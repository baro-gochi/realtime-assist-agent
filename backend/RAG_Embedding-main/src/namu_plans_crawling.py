"""
ë‚˜ë¬´ìœ„í‚¤ KT ìš”ê¸ˆì œ í¬ë¡¤ëŸ¬ v2
============================
ë‚˜ë¬´ìœ„í‚¤ì˜ KT ìš”ê¸ˆì œ í˜ì´ì§€ì—ì„œ ìš”ê¸ˆì œ ì •ë³´ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
ê³„ì¸µ êµ¬ì¡°(h3 > h4 > h5)ë¥¼ ìœ ì§€í•˜ì—¬ ìš”ê¸ˆì œë¥¼ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.

URL: https://namu.wiki/w/KT/ìš”ê¸ˆì œ

ì„¤ì¹˜:
    pip install requests beautifulsoup4 selenium webdriver-manager
"""

import requests
from bs4 import BeautifulSoup, NavigableString
import json
import time
import re
from typing import List, Dict, Optional
from dataclasses import dataclass

# Selenium ê´€ë ¨
try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from webdriver_manager.chrome import ChromeDriverManager
    SELENIUM_AVAILABLE = True
except ImportError:
    SELENIUM_AVAILABLE = False
    print("âš ï¸ Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


class NamuWikiKTCrawler:
    """ë‚˜ë¬´ìœ„í‚¤ KT ìš”ê¸ˆì œ í¬ë¡¤ëŸ¬ (ê³„ì¸µ êµ¬ì¡° ì§€ì›)"""
    
    URL = "https://namu.wiki/w/KT/%EC%9A%94%EA%B8%88%EC%A0%9C"
    
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Connection': 'keep-alive',
    }
    
    def __init__(self, use_selenium: bool = True):
        self.use_selenium = use_selenium and SELENIUM_AVAILABLE
        self.driver = None
        self.results: List[Dict] = []
        
    # ========== Selenium ê´€ë¦¬ ==========
    
    def _init_selenium(self):
        if not SELENIUM_AVAILABLE:
            raise ImportError("Seleniumì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument(f'user-agent={self.HEADERS["User-Agent"]}')
        
        service = Service(ChromeDriverManager().install())
        self.driver = webdriver.Chrome(service=service, options=chrome_options)
        self.driver.implicitly_wait(10)
        print("âœ… Selenium WebDriver ì´ˆê¸°í™” ì™„ë£Œ")
        
    def _close_selenium(self):
        if self.driver:
            self.driver.quit()
            self.driver = None
    
    # ========== í˜ì´ì§€ ìš”ì²­ ==========
    
    def _fetch_with_selenium(self, url: str) -> Optional[str]:
        try:
            if not self.driver:
                self._init_selenium()
            
            print(f"ğŸŒ í˜ì´ì§€ ë¡œë”© ì¤‘: {url}")
            self.driver.get(url)
            
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "table"))
            )
            time.sleep(3)
            
            last_height = self.driver.execute_script("return document.body.scrollHeight")
            while True:
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                time.sleep(1)
                new_height = self.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break
                last_height = new_height
            
            print("âœ… í˜ì´ì§€ ë¡œë”© ì™„ë£Œ")
            return self.driver.page_source
            
        except Exception as e:
            print(f"âŒ Selenium ì‹¤íŒ¨: {e}")
            return None
    
    def fetch_page(self, url: str = None) -> Optional[str]:
        if url is None:
            url = self.URL
        return self._fetch_with_selenium(url)
    
    # ========== ìœ í‹¸ë¦¬í‹° ==========
    
    def _clean_text(self, text: str) -> str:
        text = re.sub(r'\[.*?\]', '', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip()
    
    def _extract_cell_text(self, cell) -> str:
        content_div = cell.select_one('div.IBdgNaCn')
        if content_div:
            return self._clean_text(content_div.get_text(strip=True))
        return self._clean_text(cell.get_text(strip=True))
    
    def _get_heading_name(self, heading) -> str:
        """h3/h4/h5 íƒœê·¸ì—ì„œ ì‹¤ì œ ì´ë¦„ ì¶”ì¶œ"""
        for span in heading.select('span[id]'):
            name = span.get('id', '')
            if name and not name.startswith('s-') and not name.startswith('rfn-'):
                return name
        return ''
    
    # ========== í…Œì´ë¸” íŒŒì‹± ==========
    
    def _parse_table(self, table) -> Dict:
        """ë‹¨ì¼ í…Œì´ë¸” - HTML ì›ë³¸ìœ¼ë¡œ ì €ì¥"""
        
        # í…Œì´ë¸” ì´ë¦„ ì¶”ì¶œ (ì²« ë²ˆì§¸ í–‰ì—ì„œ)
        table_name = ''
        first_row = table.select_one('tr.R4S-40tq')
        if first_row:
            first_cell = first_row.select_one('td')
            if first_cell:
                table_name = self._extract_cell_text(first_cell)
        
        # HTML ì›ë³¸ ì €ì¥ (ì •ë¦¬ëœ ë²„ì „)
        table_html = self._clean_table_html(table)
        
        result = {
            'table_name': table_name,
            'table_html': table_html
        }
        
        return result
    
    def _clean_table_html(self, table) -> str:
        """í…Œì´ë¸” HTML ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ì†ì„± ì œê±°)"""
        import copy
        
        # í…Œì´ë¸” ë³µì‚¬ë³¸ ìƒì„±
        table_copy = copy.copy(table)
        
        # ì›ë³¸ HTML ë°˜í™˜ (ë¬¸ìì—´ë¡œ)
        html_str = str(table)
        
        # ë‚˜ë¬´ìœ„í‚¤ íŠ¹ìœ ì˜ data-v-cf63095b ë“± ì œê±° (ì„ íƒì )
        html_str = re.sub(r'\s*data-v-[a-z0-9]+=""', '', html_str)
        html_str = re.sub(r'\s*data-dark-style="[^"]*"', '', html_str)
        
        return html_str
    
    def _extract_description_after_table(self, table) -> str:
        """í…Œì´ë¸” ë‹¤ìŒì˜ ì„¤ëª… í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        descriptions = []
        parent = table.find_parent('div', class_='pCELUZmY')
        if parent:
            next_div = parent.find_next_sibling('div', class_='IBdgNaCn')
            if next_div:
                text = self._clean_text(next_div.get_text(strip=True))
                if text:
                    descriptions.append(text)
        return ' '.join(descriptions)
    
    # ========== ê³„ì¸µ êµ¬ì¡° íŒŒì‹± ==========
    
    def _parse_hierarchical_structure(self, soup: BeautifulSoup) -> List[Dict]:
        """h3 > h4 > h5 ê³„ì¸µ êµ¬ì¡° íŒŒì‹±
        
        HTML êµ¬ì¡°:
        div (h3ì™€ ê°™ì€ ë ˆë²¨)
        div (h4ì™€ ê°™ì€ ë ˆë²¨) 
        div (h5ì™€ ê°™ì€ ë ˆë²¨)
        div (í…Œì´ë¸” - h5ì™€ ê°™ì€ ë ˆë²¨ì˜ í˜•ì œ ìš”ì†Œ)
        """
        
        # ë¬¸ì„œì˜ ëª¨ë“  ì£¼ìš” ìš”ì†Œë¥¼ ìˆœì„œëŒ€ë¡œ ìˆ˜ì§‘ (í—¤ë”© + í…Œì´ë¸”)
        # ìµœìƒìœ„ ì»¨í…Œì´ë„ˆì—ì„œ ìˆœì°¨ì ìœ¼ë¡œ íƒìƒ‰
        all_elements = []
        
        # ëª¨ë“  h3, h4, h5, tableì„ í¬í•¨í•˜ëŠ” divë“¤ì„ ì°¾ìŒ
        for elem in soup.select('h3.PVbZbzR7, h4.PVbZbzR7, h5.PVbZbzR7, table._3lpnOiRq'):
            all_elements.append(elem)
        
        # ë¬¸ì„œ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (sourceline ê¸°ì¤€)
        # BeautifulSoupì—ì„œëŠ” ìš”ì†Œ ìˆœì„œê°€ ì´ë¯¸ ë¬¸ì„œ ìˆœì„œ
        
        # ê³„ì¸µ êµ¬ì¡°ë¡œ ì¡°ì§í™”
        h3_sections = []
        current_h3 = None
        current_h4 = None
        current_h5 = None
        
        for elem in all_elements:
            if elem.name in ['h3', 'h4', 'h5']:
                # í—¤ë”© ì²˜ë¦¬
                name = self._get_heading_name(elem)
                if not name:
                    continue
                
                level = elem.name
                
                if level == 'h3':
                    # ì´ì „ h5 ì €ì¥
                    if current_h5 and current_h4:
                        current_h4['plans'].append(current_h5)
                    # ì´ì „ h4 ì €ì¥
                    if current_h4 and current_h3:
                        current_h3['sub_categories'].append(current_h4)
                    # ì´ì „ h3 ì €ì¥
                    if current_h3:
                        h3_sections.append(current_h3)
                    
                    current_h3 = {
                        'category': name,
                        'sub_categories': [],
                        'direct_tables': []
                    }
                    current_h4 = None
                    current_h5 = None
                    
                elif level == 'h4':
                    # ì´ì „ h5 ì €ì¥
                    if current_h5 and current_h4:
                        current_h4['plans'].append(current_h5)
                    # ì´ì „ h4 ì €ì¥
                    if current_h4 and current_h3:
                        current_h3['sub_categories'].append(current_h4)
                    
                    current_h4 = {
                        'name': name,
                        'plans': [],
                        'direct_tables': []
                    }
                    current_h5 = None
                    
                elif level == 'h5':
                    # ì´ì „ h5 ì €ì¥
                    if current_h5 and current_h4:
                        current_h4['plans'].append(current_h5)
                    
                    # h4ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ h4 ìƒì„±
                    if current_h4 is None:
                        current_h4 = {
                            'name': 'ê¸°íƒ€',
                            'plans': [],
                            'direct_tables': []
                        }
                    
                    current_h5 = {
                        'plan_name': name,
                        'tables': []
                    }
                    
            elif elem.name == 'table':
                # í…Œì´ë¸” ì²˜ë¦¬
                parsed = self._parse_table(elem)
                parsed['description'] = self._extract_description_after_table(elem)
                
                # í˜„ì¬ h5ê°€ ìˆìœ¼ë©´ h5ì— ì¶”ê°€
                if current_h5 is not None:
                    current_h5['tables'].append(parsed)
                # h5ê°€ ì—†ê³  h4ê°€ ìˆìœ¼ë©´ h4 ì§ì† í…Œì´ë¸”
                elif current_h4 is not None:
                    current_h4['direct_tables'].append(parsed)
                # h4ë„ ì—†ê³  h3ë§Œ ìˆìœ¼ë©´ h3 ì§ì† í…Œì´ë¸”
                elif current_h3 is not None:
                    current_h3['direct_tables'].append(parsed)
        
        # ë§ˆì§€ë§‰ ë°ì´í„° ì €ì¥
        if current_h5 and current_h4:
            current_h4['plans'].append(current_h5)
        if current_h4 and current_h3:
            current_h3['sub_categories'].append(current_h4)
        if current_h3:
            h3_sections.append(current_h3)
        
        return h3_sections
    
    # ========== ë©”ì¸ í¬ë¡¤ë§ ==========
    
    def parse_page(self, html: str) -> List[Dict]:
        """HTML íŒŒì‹±í•˜ì—¬ ê³„ì¸µ êµ¬ì¡°ë¡œ ìš”ê¸ˆì œ ì •ë³´ ì¶”ì¶œ"""
        soup = BeautifulSoup(html, 'html.parser')
        
        print("\nğŸ” ê³„ì¸µ êµ¬ì¡° ë¶„ì„ ì¤‘ (h3 > h4 > h5)...")
        
        results = self._parse_hierarchical_structure(soup)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        for h3_section in results:
            print(f"\nğŸ“ {h3_section['category']}")
            for h4_section in h3_section.get('sub_categories', []):
                plan_count = len(h4_section.get('plans', []))
                direct_count = len(h4_section.get('direct_tables', []))
                print(f"   ğŸ“‚ {h4_section['name']} (ìš”ê¸ˆì œ {plan_count}ê°œ, ì§ì† í…Œì´ë¸” {direct_count}ê°œ)")
                
                for plan in h4_section.get('plans', []):
                    table_count = len(plan.get('tables', []))
                    print(f"      âœ… {plan['plan_name']} (í…Œì´ë¸” {table_count}ê°œ)")
        
        return results
    
    def crawl(self, html: str = None) -> List[Dict]:
        """í¬ë¡¤ë§ ì‹¤í–‰"""
        print("\n" + "="*60)
        print("ğŸš€ ë‚˜ë¬´ìœ„í‚¤ KT ìš”ê¸ˆì œ í¬ë¡¤ë§ ì‹œì‘ (v2 - ê³„ì¸µêµ¬ì¡°)")
        print("="*60)
        
        try:
            if html is None:
                html = self.fetch_page()
            
            if not html:
                print("âŒ HTMLì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            self.results = self.parse_page(html)
            
            # í†µê³„
            total_h4 = 0
            total_plans = 0
            total_tables = 0
            
            for h3 in self.results:
                for h4 in h3.get('sub_categories', []):
                    total_h4 += 1
                    total_tables += len(h4.get('direct_tables', []))
                    for plan in h4.get('plans', []):
                        total_plans += 1
                        total_tables += len(plan.get('tables', []))
            
            print("\n" + "="*60)
            print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
            print(f"   ğŸ“ ëŒ€ë¶„ë¥˜(h3): {len(self.results)}ê°œ")
            print(f"   ğŸ“‚ ì¤‘ë¶„ë¥˜(h4): {total_h4}ê°œ")
            print(f"   ğŸ“„ ìš”ê¸ˆì œ(h5): {total_plans}ê°œ")
            print(f"   ğŸ“Š ì´ í…Œì´ë¸”: {total_tables}ê°œ")
            print("="*60)
            
            return self.results
            
        finally:
            self._close_selenium()
    
    def crawl_from_file(self, filepath: str) -> List[Dict]:
        """ë¡œì»¬ HTML íŒŒì¼ì—ì„œ í¬ë¡¤ë§"""
        print(f"\nğŸ“‚ íŒŒì¼ì—ì„œ HTML ë¡œë”©: {filepath}")
        
        with open(filepath, 'r', encoding='utf-8') as f:
            html = f.read()
        
        return self.crawl(html)
    
    # ========== ì €ì¥ ==========
    
    def save_to_json(self, filename: str = 'namu_kt_plans.json'):
        """JSONìœ¼ë¡œ ì €ì¥"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSON ì €ì¥: {filename}")
        return filename
    
    def print_summary(self):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        if not self.results:
            print("âš ï¸ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("\n" + "="*60)
        print("ğŸ“Š KT ìš”ê¸ˆì œ ìˆ˜ì§‘ ê²°ê³¼ (ê³„ì¸µ êµ¬ì¡°)")
        print("="*60)
        
        for h3_section in self.results:
            print(f"\n{'='*50}")
            print(f"ğŸ“ {h3_section['category']}")
            print(f"{'='*50}")
            
            for h4_section in h3_section.get('sub_categories', []):
                print(f"\n  ğŸ“‚ {h4_section['name']}")
                print(f"  {'-'*40}")
                
                # h4 ì§ì† í…Œì´ë¸” (h5ê°€ ì—†ëŠ” ê²½ìš°)
                for table in h4_section.get('direct_tables', []):
                    print(f"\n    ğŸ“‹ {table.get('table_name', '(í…Œì´ë¸”)')}")
                    html_len = len(table.get('table_html', ''))
                    print(f"       HTML ê¸¸ì´: {html_len} chars")
                
                # h5 ìš”ê¸ˆì œë“¤
                for plan in h4_section.get('plans', []):
                    print(f"\n    ğŸ·ï¸ {plan['plan_name']}")
                    
                    # í•´ë‹¹ ìš”ê¸ˆì œì˜ ëª¨ë“  í…Œì´ë¸”
                    for idx, table in enumerate(plan.get('tables', []), 1):
                        if len(plan.get('tables', [])) > 1:
                            print(f"       [í…Œì´ë¸” {idx}] {table.get('table_name', '')}")
                        else:
                            print(f"       í…Œì´ë¸”: {table.get('table_name', '')}")
                        html_len = len(table.get('table_html', ''))
                        print(f"       HTML ê¸¸ì´: {html_len} chars")
                        if table.get('description'):
                            print(f"       ğŸ“ {table['description'][:50]}...")


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    print("="*60)
    print("ğŸ¢ ë‚˜ë¬´ìœ„í‚¤ KT ìš”ê¸ˆì œ í¬ë¡¤ëŸ¬ v2 (ê³„ì¸µêµ¬ì¡°)")
    print("="*60)
    print(f"\nğŸ“ URL: {NamuWikiKTCrawler.URL}")
    print("\nğŸ“‹ êµ¬ì¡°: h3(ëŒ€ë¶„ë¥˜) > h4(ì¤‘ë¶„ë¥˜) > h5(ìš”ê¸ˆì œ) > í…Œì´ë¸”ë“¤")
    
    print("\n" + "-"*60)
    print("ì‹¤í–‰ ë°©ë²• ì„ íƒ:")
    print("  1. Seleniumìœ¼ë¡œ ì§ì ‘ í¬ë¡¤ë§")
    print("  2. ë¡œì»¬ HTML íŒŒì¼ì—ì„œ í¬ë¡¤ë§")
    
    choice = input("\nì„ íƒ (1/2): ").strip()
    
    crawler = NamuWikiKTCrawler(use_selenium=True)
    
    if choice == '1':
        results = crawler.crawl()
    elif choice == '2':
        filepath = input("HTML íŒŒì¼ ê²½ë¡œ: ").strip()
        results = crawler.crawl_from_file(filepath)
    else:
        print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")
        return
    
    if results:
        crawler.print_summary()
        crawler.save_to_json()
    
    print("\nâœ… ì™„ë£Œ!")


if __name__ == '__main__':
    main()