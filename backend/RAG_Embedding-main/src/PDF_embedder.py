from unstructured.partition.pdf import partition_pdf
import re
from openai import OpenAI
import chromadb
from chromadb.utils import embedding_functions
import os
from glob import glob
from dotenv import load_dotenv
import json
import tiktoken
import asyncio
import aiohttp
from concurrent.futures import ThreadPoolExecutor
from typing import List, Dict, Optional, Tuple
import time

# í™˜ê²½ì„¤ì •
load_dotenv()
PDF_DIRECTORY = os.getenv("DIR")
OPENAI_API_KEY = os.getenv("API_KEY")
CLASSIFICATION_CATEGORIES = os.getenv("CATEGORIES")
CHROMA_DB_PATH = os.getenv("CHROMA_DB_PATH")
COLLECTION_NAME = os.getenv("COLLECTION_NAME")

# ì²­í‚¹ ì„¤ì •
MAX_CHUNK_SIZE = 1500
MIN_CHUNK_SIZE = 30
CHUNK_OVERLAP = 100

# ì„ë² ë”© ì„¤ì •
EMBEDDING_MODEL = "text-embedding-3-large"
MAX_EMBEDDING_TOKENS = 8000
EMBEDDING_DIMENSIONS = 3072  # text-embedding-3-large ì°¨ì›

# ê³ ê¸‰ ì„ë² ë”© ì„¤ì •
CONTEXTUAL_MODEL = "gpt-4o-mini"  # ì»¨í…ìŠ¤íŠ¸ ìƒì„±ìš© ëª¨ë¸
HYDE_MODEL = "gpt-4o-mini"  # ê°€ìƒ ì§ˆë¬¸ ìƒì„±ìš© ëª¨ë¸
MAX_CONCURRENT_REQUESTS = 10  # ë™ì‹œ API ìš”ì²­ ìˆ˜

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=OPENAI_API_KEY)

# ========== í† í° ê´€ë ¨ ìœ í‹¸ë¦¬í‹° ==========
def count_tokens(text, model="text-embedding-3-large"):
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
        return len(encoding.encode(text))
    except Exception:
        # ëŒ€ëµì ì¸ ê³„ì‚° (í•œê¸€ ê¸°ì¤€ ì•½ 1.5ìë‹¹ 1í† í°)
        return len(text) // 2


def truncate_text(text, max_tokens=8000):
    """í† í° ì œí•œì— ë§ê²Œ í…ìŠ¤íŠ¸ ìë¥´ê¸°"""
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
        tokens = encoding.encode(text)
        if len(tokens) > max_tokens:
            truncated_tokens = tokens[:max_tokens]
            return encoding.decode(truncated_tokens)
        return text
    except Exception:
        # ëŒ€ëµì ì¸ ìë¥´ê¸°
        max_chars = max_tokens * 2
        return text[:max_chars]


def split_text_by_tokens(text, max_tokens=8000, overlap_tokens=200):
    """
    í† í° ì œí•œì— ë§ê²Œ í…ìŠ¤íŠ¸ë¥¼ ì—¬ëŸ¬ ì²­í¬ë¡œ ë¶„í• 
    - ì˜¤ë²„ë©ì„ ì ìš©í•˜ì—¬ ë¬¸ë§¥ ìœ ì§€
    """
    try:
        encoding = tiktoken.get_encoding("cl100k_base")
        tokens = encoding.encode(text)
        
        # í† í° ìˆ˜ê°€ ì œí•œ ì´í•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        if len(tokens) <= max_tokens:
            return [text]
        
        # í† í° ë‹¨ìœ„ë¡œ ë¶„í• 
        chunks = []
        start = 0
        
        while start < len(tokens):
            end = min(start + max_tokens, len(tokens))
            chunk_tokens = tokens[start:end]
            chunk_text = encoding.decode(chunk_tokens)
            chunks.append(chunk_text)
            
            # ë‹¤ìŒ ì‹œì‘ì  (ì˜¤ë²„ë© ì ìš©)
            start = end - overlap_tokens if end < len(tokens) else end
        
        return chunks
        
    except Exception as e:
        print(f"   âš ï¸ í† í° ë¶„í•  ì‹¤íŒ¨: {e}")
        # ëŒ€ëµì ì¸ ë¬¸ì ê¸°ë°˜ ë¶„í• 
        max_chars = max_tokens * 2
        overlap_chars = overlap_tokens * 2
        chunks = []
        start = 0
        
        while start < len(text):
            end = min(start + max_chars, len(text))
            chunks.append(text[start:end])
            start = end - overlap_chars if end < len(text) else end
        
        return chunks


def clean_text(text):
    """í…ìŠ¤íŠ¸ ì •ì œ (ë¹ˆ ë¬¸ìì—´, íŠ¹ìˆ˜ë¬¸ì ì²˜ë¦¬)"""
    if not text:
        return ""
    # None ì²´í¬
    if text is None:
        return ""
    # ê³µë°±ë§Œ ìˆëŠ” ê²½ìš°
    text = str(text).strip()
    if not text:
        return ""
    # ì œì–´ ë¬¸ì ì œê±°
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    return text


# ========== ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ìœ í‹¸ë¦¬í‹° ==========
def element_to_markdown(element):
    """Unstructured elementë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    if not hasattr(element, 'text') or not element.text:
        return ""

    text = element.text.strip()
    if not text:
        return ""

    category = getattr(element, 'category', 'NarrativeText')

    # ì¹´í…Œê³ ë¦¬ë³„ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
    if category == "Title":
        # ì œëª© ë ˆë²¨ ì¶”ì • (í…ìŠ¤íŠ¸ ê¸¸ì´, í°íŠ¸ í¬ê¸° ë“± ê³ ë ¤)
        return f"## {text}\n\n"

    elif category == "Header":
        return f"### {text}\n\n"

    elif category == "Table":
        # í…Œì´ë¸”ì€ ë³„ë„ ì²˜ë¦¬ (table_to_markdown ì‚¬ìš©)
        return text + "\n\n"

    elif category == "ListItem":
        # ë²ˆí˜¸ ë˜ëŠ” ë¶ˆë¦¿ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬
        if re.match(r'^\d+[\.\)]\s*', text):
            return f"{text}\n"
        elif re.match(r'^[ê°€-í£][\.\)]\s*', text):
            return f"{text}\n"
        else:
            return f"- {text}\n"

    elif category == "FigureCaption":
        return f"*{text}*\n\n"

    elif category == "Footer" or category == "PageNumber":
        # í˜ì´ì§€ ë²ˆí˜¸, í‘¸í„°ëŠ” ìŠ¤í‚µ
        return ""

    else:
        # NarrativeText ë“± ì¼ë°˜ í…ìŠ¤íŠ¸
        return f"{text}\n\n"


def table_to_markdown_with_gpt(table_html_or_text, context=""):
    """GPTë¥¼ í™œìš©í•˜ì—¬ í…Œì´ë¸”ì„ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì„¤ëª… ì¶”ê°€"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ í‘œ(í…Œì´ë¸”) ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ í…Œì´ë¸” ë°ì´í„°ë¥¼ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:

1. **ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”**: ê¹”ë”í•œ ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” í˜•ì‹ìœ¼ë¡œ ë³€í™˜
2. **í…Œì´ë¸” ìš”ì•½**: í…Œì´ë¸”ì˜ í•µì‹¬ ë‚´ìš©ì„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½
3. **ì£¼ìš” ì •ë³´**: í…Œì´ë¸”ì—ì„œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” í•µì‹¬ ì •ë³´ (ê°€ê²©, ì¡°ê±´, í˜œíƒ ë“±)

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ:
{
    "markdown_table": "| í—¤ë”1 | í—¤ë”2 |\\n|---|---|\\n| ê°’1 | ê°’2 |",
    "summary": "ì´ í…Œì´ë¸”ì€ ... ë¥¼ ë³´ì—¬ì¤ë‹ˆë‹¤.",
    "key_info": ["ì •ë³´1", "ì •ë³´2", "ì •ë³´3"]
}"""
                },
                {
                    "role": "user",
                    "content": f"""ë‹¤ìŒ í…Œì´ë¸”ì„ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜í•˜ê³  ë¶„ì„í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸: {context[:500] if context else 'ì—†ìŒ'}

í…Œì´ë¸” ë°ì´í„°:
{table_html_or_text}"""
                }
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)

        # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ì¡°í•©
        markdown_output = ""

        if result.get("markdown_table"):
            markdown_output += result["markdown_table"] + "\n\n"

        if result.get("summary"):
            markdown_output += f"**í…Œì´ë¸” ìš”ì•½**: {result['summary']}\n\n"

        if result.get("key_info"):
            markdown_output += "**ì£¼ìš” ì •ë³´**:\n"
            for info in result["key_info"]:
                markdown_output += f"- {info}\n"
            markdown_output += "\n"

        return markdown_output

    except Exception as e:
        print(f"   âš ï¸ í…Œì´ë¸” GPT ë³€í™˜ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
        return f"```\n{table_html_or_text}\n```\n\n"


def convert_elements_to_markdown(elements, file_name=""):
    """ëª¨ë“  elementsë¥¼ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¡œ ë³€í™˜"""
    markdown_parts = []
    current_context = ""  # í…Œì´ë¸” ì£¼ë³€ ë¬¸ë§¥ ì €ì¥

    # íŒŒì¼ëª…ì„ ìµœìƒìœ„ ì œëª©ìœ¼ë¡œ
    if file_name:
        markdown_parts.append(f"# {file_name}\n\n")

    for i, element in enumerate(elements):
        if not hasattr(element, 'text') or not element.text:
            continue

        category = getattr(element, 'category', 'NarrativeText')
        text = element.text.strip()

        if not text:
            continue

        # í…Œì´ë¸” ì²˜ë¦¬
        if category == "Table":
            # HTML í…Œì´ë¸”ì´ ìˆìœ¼ë©´ ì‚¬ìš©
            table_content = text
            if hasattr(element, 'metadata') and hasattr(element.metadata, 'text_as_html'):
                table_content = element.metadata.text_as_html or text

            # GPTë¡œ í…Œì´ë¸” ë³€í™˜
            table_markdown = table_to_markdown_with_gpt(table_content, current_context)
            markdown_parts.append(table_markdown)
        else:
            # ì¼ë°˜ ìš”ì†Œ ë§ˆí¬ë‹¤ìš´ ë³€í™˜
            md = element_to_markdown(element)
            if md:
                markdown_parts.append(md)
                # ì»¨í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸ (ìµœê·¼ 500ì)
                current_context = (current_context + " " + text)[-500:]

    return "".join(markdown_parts)


def filter_appendix_sections(markdown_text: str) -> str:
    """
    ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œì—ì„œ 'ë¶€ì¹™' ì„¹ì…˜ì„ í•„í„°ë§í•˜ì—¬ ë§ˆì§€ë§‰ ë¶€ì¹™ë§Œ ë‚¨ê¹€

    ë¶€ì¹™ì´ ì—¬ëŸ¬ ê°œ ìˆëŠ” ê²½ìš°:
    - ë§ˆì§€ë§‰ ë¶€ì¹™ê³¼ ê·¸ ë‚´ìš©ë§Œ ìœ ì§€
    - ì´ì „ ë¶€ì¹™ë“¤ì€ ëª¨ë‘ ì œê±°

    Args:
        markdown_text: ë§ˆí¬ë‹¤ìš´ ë³€í™˜ëœ ì „ì²´ ë¬¸ì„œ

    Returns:
        í•„í„°ë§ëœ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
    """
    if not markdown_text:
        return markdown_text

    # ë¶€ì¹™ íŒ¨í„´: "ë¶€ì¹™", "ë¶€ ì¹™", "é™„å‰‡", "## ë¶€ì¹™", "### ë¶€ì¹™" ë“±
    # ë‚ ì§œë‚˜ ë²ˆí˜¸ê°€ ë¶™ëŠ” ê²½ìš°ë„ í¬í•¨: "ë¶€ì¹™ (2024.01.01)", "ë¶€ì¹™ <ì œ1í˜¸>"
    appendix_pattern = re.compile(
        r'^(#{1,6}\s*)?(ë¶€\s*ì¹™|é™„\s*å‰‡)(\s*[\(<\[ã€].*?[\)>\]ã€‘])?(\s*$|\s+)',
        re.MULTILINE | re.IGNORECASE
    )

    # ëª¨ë“  ë¶€ì¹™ ìœ„ì¹˜ ì°¾ê¸°
    matches = list(appendix_pattern.finditer(markdown_text))

    if len(matches) <= 1:
        # ë¶€ì¹™ì´ ì—†ê±°ë‚˜ 1ê°œë§Œ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        return markdown_text

    print(f"   ğŸ“‹ ë¶€ì¹™ {len(matches)}ê°œ ë°œê²¬ â†’ ë§ˆì§€ë§‰ ë¶€ì¹™ë§Œ ìœ ì§€")

    # ë§ˆì§€ë§‰ ë¶€ì¹™ì˜ ì‹œì‘ ìœ„ì¹˜
    last_appendix_start = matches[-1].start()

    # ì²« ë²ˆì§¸ ë¶€ì¹™ë¶€í„° ë§ˆì§€ë§‰ ë¶€ì¹™ ì§ì „ê¹Œì§€ì˜ ë‚´ìš©ì„ ì œê±°
    first_appendix_start = matches[0].start()

    # ë¶€ì¹™ ì´ì „ ë‚´ìš© + ë§ˆì§€ë§‰ ë¶€ì¹™ ë‚´ìš©
    content_before_appendix = markdown_text[:first_appendix_start]
    last_appendix_content = markdown_text[last_appendix_start:]

    filtered_text = content_before_appendix + last_appendix_content

    # ì œê±°ëœ ë¶€ì¹™ ìˆ˜ ì¶œë ¥
    removed_count = len(matches) - 1
    print(f"      âœ‚ï¸ {removed_count}ê°œ ë¶€ì¹™ ì œê±°ë¨")

    return filtered_text


# ========== 1. PDF íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ==========
def get_pdf_files(directory):
    """ë””ë ‰í„°ë¦¬ ë‚´ ëª¨ë“  PDF íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    pdf_pattern = os.path.join(directory, "**", "*.pdf")
    pdf_files = glob(pdf_pattern, recursive=True)
    
    print(f"ğŸ“ ë””ë ‰í„°ë¦¬: {directory}")
    print(f"ğŸ“„ ë°œê²¬ëœ PDF íŒŒì¼: {len(pdf_files)}ê°œ")
    for f in pdf_files:
        print(f"   - {f}")
    
    return pdf_files


# ========== 2. PDF ì¶”ì¶œ ==========
def extract_elements(file_path):
    """PDFì—ì„œ elements ì¶”ì¶œ"""
    print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘: {file_path}")
    try:
        elements = partition_pdf(
            filename=file_path,
            strategy="hi_res",
            infer_table_structure=True,
            languages=["kor"]
        )
        return elements
    except Exception as e:
        print(f"   âš ï¸ PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []


# ========== 3. ë¬¸ì„œ êµ¬ì¡° ê°ì§€ ==========
def detect_document_structure(elements):
    """ë¬¸ì„œ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì í•©í•œ ì²­í‚¹ ë°©ì‹ ê²°ì •"""
    full_text = " ".join([el.text for el in elements if hasattr(el, 'text')])
    
    # íŒ¨í„´ ì •ì˜
    patterns = {
        "article": re.compile(r"ì œ\s*\d+\s*ì¡°"),           # ì œ1ì¡°, ì œ 2 ì¡°
        "chapter": re.compile(r"ì œ\s*\d+\s*ì¥"),           # ì œ1ì¥, ì œ 2 ì¥
        "number_dot": re.compile(r"^\d+\.\s", re.MULTILINE),  # 1. 2. 3.
        "korean_number": re.compile(r"^[ê°€-í£]\.\s", re.MULTILINE),  # ê°€. ë‚˜. ë‹¤.
        "qa_pattern": re.compile(r"(Q\d*[\.:]\s*|A\d*[\.:]\s*|ì§ˆë¬¸\s*:|\ë‹µë³€\s*:)", re.MULTILINE),  # Q: A: ì§ˆë¬¸: ë‹µë³€:
        "bracket_number": re.compile(r"[\[ã€]\d+[\]ã€‘]"),   # [1] ã€2ã€‘
    }
    
    # ê° íŒ¨í„´ ë§¤ì¹­ íšŸìˆ˜ ê³„ì‚°
    matches = {}
    for name, pattern in patterns.items():
        matches[name] = len(pattern.findall(full_text))
    
    # Title ìš”ì†Œ ê°œìˆ˜ í™•ì¸
    title_count = sum(1 for el in elements if hasattr(el, 'category') and el.category == "Title")
    matches["title_elements"] = title_count
    
    print(f"   ğŸ“Š êµ¬ì¡° ë¶„ì„: {matches}")
    
    # ì²­í‚¹ ë°©ì‹ ê²°ì •
    if matches["article"] >= 3:
        return "article"  # ì¡°/ì¥ ê¸°ë°˜
    elif matches["qa_pattern"] >= 3:
        return "qa"  # Q&A ê¸°ë°˜
    elif title_count >= 3:
        return "title"  # ì œëª© ìš”ì†Œ ê¸°ë°˜
    elif matches["number_dot"] >= 5 or matches["korean_number"] >= 5:
        return "numbered"  # ë²ˆí˜¸ ê¸°ë°˜
    elif matches["bracket_number"] >= 3:
        return "bracket"  # ê´„í˜¸ ë²ˆí˜¸ ê¸°ë°˜
    else:
        return "semantic"  # ì‹œë§¨í‹± (í´ë°±)


# ========== 4. ì¡°/ì¥ ê¸°ë°˜ ì²­í‚¹ ==========
def chunk_by_article(elements, file_name, file_path):
    """ì¡°(Article) ë‹¨ìœ„ë¡œ ì²­í‚¹"""
    chunks = []
    current_chunk = {
        "title": None,
        "chapter": None,
        "content": [],
        "metadata": {}
    }
    
    chapter_pattern = re.compile(r"ì œ\s*\d+\s*ì¥")
    article_pattern = re.compile(r"ì œ\s*\d+\s*ì¡°")
    
    current_chapter = None
    
    for el in elements:
        if not hasattr(el, 'text'):
            continue
        text = el.text.strip()
        if not text:
            continue
        
        if chapter_pattern.search(text):
            current_chapter = text
            continue
        
        if article_pattern.search(text):
            if current_chunk["title"] and current_chunk["content"]:
                current_chunk["content"] = "\n".join(current_chunk["content"])
                chunks.append(current_chunk)
            
            current_chunk = {
                "title": text,
                "chapter": current_chapter,
                "content": [],
                "metadata": {
                    "category": file_name,
                    "source": file_path,
                    "page_number": el.metadata.page_number if hasattr(el.metadata, 'page_number') else None,
                    "chunk_type": "article"
                }
            }
        else:
            current_chunk["content"].append(text)
    
    if current_chunk["title"] and current_chunk["content"]:
        current_chunk["content"] = "\n".join(current_chunk["content"])
        chunks.append(current_chunk)
    
    return chunks


# ========== 5. ì œëª©(Title) ìš”ì†Œ ê¸°ë°˜ ì²­í‚¹ ==========
def chunk_by_title(elements, file_name, file_path):
    """Unstructuredì˜ Title ìš”ì†Œ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹"""
    chunks = []
    current_chunk = {
        "title": None,
        "chapter": None,
        "content": [],
        "metadata": {}
    }
    
    for el in elements:
        if not hasattr(el, 'text'):
            continue
        text = el.text.strip()
        if not text:
            continue
        
        # Title ìš”ì†Œë¥¼ ìƒˆ ì²­í¬ì˜ ì‹œì‘ì ìœ¼ë¡œ
        if hasattr(el, 'category') and el.category == "Title":
            if current_chunk["content"]:
                current_chunk["content"] = "\n".join(current_chunk["content"])
                if not current_chunk["title"]:
                    current_chunk["title"] = current_chunk["content"][:50] + "..."
                chunks.append(current_chunk)
            
            current_chunk = {
                "title": text,
                "chapter": None,
                "content": [],
                "metadata": {
                    "category": file_name,
                    "source": file_path,
                    "page_number": el.metadata.page_number if hasattr(el.metadata, 'page_number') else None,
                    "chunk_type": "title"
                }
            }
        else:
            current_chunk["content"].append(text)
    
    if current_chunk["content"]:
        current_chunk["content"] = "\n".join(current_chunk["content"])
        if not current_chunk["title"]:
            current_chunk["title"] = current_chunk["content"][:50] + "..."
        chunks.append(current_chunk)
    
    return chunks


# ========== 6. Q&A ê¸°ë°˜ ì²­í‚¹ ==========
def chunk_by_qa(elements, file_name, file_path):
    """Q&A íŒ¨í„´ ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹"""
    chunks = []
    full_text = "\n".join([el.text for el in elements if hasattr(el, 'text') and el.text])
    
    # Q&A íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
    qa_pattern = re.compile(r'(Q\d*[\.:]\s*|ì§ˆë¬¸\s*[\d]*[\.:]*\s*)', re.MULTILINE | re.IGNORECASE)
    parts = qa_pattern.split(full_text)
    
    current_q = None
    for i, part in enumerate(parts):
        part = part.strip()
        if not part:
            continue
        
        if qa_pattern.match(part + " "):
            continue
        
        # Që¡œ ì‹œì‘í•˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
        if i > 0 and qa_pattern.match(parts[i-1] if i-1 < len(parts) else ""):
            current_q = part
        elif current_q:
            # Qì™€ Aë¥¼ í•©ì³ì„œ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
            chunk = {
                "title": f"Q: {current_q[:50]}..." if len(current_q) > 50 else f"Q: {current_q}",
                "chapter": None,
                "content": f"ì§ˆë¬¸: {current_q}\në‹µë³€: {part}",
                "metadata": {
                    "category": file_name,
                    "source": file_path,
                    "page_number": None,
                    "chunk_type": "qa"
                }
            }
            chunks.append(chunk)
            current_q = None
    
    # Q&A íŒ¨í„´ì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šìœ¼ë©´ ì‹œë§¨í‹± ì²­í‚¹ìœ¼ë¡œ í´ë°±
    if len(chunks) < 2:
        return chunk_semantic(elements, file_name, file_path)
    
    return chunks


# ========== 7. ë²ˆí˜¸ ê¸°ë°˜ ì²­í‚¹ ==========
def chunk_by_number(elements, file_name, file_path):
    """ë²ˆí˜¸ íŒ¨í„´ (1. 2. ê°€. ë‚˜.) ê¸°ì¤€ìœ¼ë¡œ ì²­í‚¹"""
    chunks = []
    full_text = "\n".join([el.text for el in elements if hasattr(el, 'text') and el.text])
    
    # ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ ë¶„í• 
    number_pattern = re.compile(r'\n(?=\d+\.\s|[ê°€-í£]\.\s)')
    parts = number_pattern.split(full_text)
    
    for i, part in enumerate(parts):
        part = part.strip()
        if len(part) < MIN_CHUNK_SIZE:
            continue
        
        # ì²« ì¤„ì„ ì œëª©ìœ¼ë¡œ ì‚¬ìš©
        lines = part.split('\n')
        title = lines[0][:50] + "..." if len(lines[0]) > 50 else lines[0]
        
        chunk = {
            "title": title,
            "chapter": None,
            "content": part,
            "metadata": {
                "category": file_name,
                "source": file_path,
                "page_number": None,
                "chunk_type": "numbered"
            }
        }
        chunks.append(chunk)
    
    if len(chunks) < 2:
        return chunk_semantic(elements, file_name, file_path)
    
    return chunks


# ========== 8. ì‹œë§¨í‹± ì²­í‚¹ (í´ë°±) ==========
def chunk_semantic(elements, file_name, file_path):
    """ê³ ì • í¬ê¸° + ë¬¸ë‹¨ ê¸°ë°˜ ì‹œë§¨í‹± ì²­í‚¹"""
    chunks = []
    full_text = "\n".join([el.text for el in elements if hasattr(el, 'text') and el.text])

    if not full_text.strip():
        return chunks

    # ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë¶„í• 
    paragraphs = re.split(r'\n\s*\n', full_text)

    current_chunk = []
    current_length = 0

    for para in paragraphs:
        para = para.strip()
        if not para:
            continue

        para_length = len(para)

        # í˜„ì¬ ì²­í¬ + ìƒˆ ë¬¸ë‹¨ì´ ìµœëŒ€ í¬ê¸°ë¥¼ ì´ˆê³¼í•˜ë©´
        if current_length + para_length > MAX_CHUNK_SIZE and current_chunk:
            chunk_text = "\n\n".join(current_chunk)
            title = chunk_text[:50] + "..." if len(chunk_text) > 50 else chunk_text

            chunk = {
                "title": title,
                "chapter": None,
                "content": chunk_text,
                "metadata": {
                    "category": file_name,
                    "source": file_path,
                    "page_number": None,
                    "chunk_type": "semantic"
                }
            }
            chunks.append(chunk)

            # ì˜¤ë²„ë© ì ìš©
            current_chunk = [para]
            current_length = para_length
        else:
            current_chunk.append(para)
            current_length += para_length

    # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
    if current_chunk:
        chunk_text = "\n\n".join(current_chunk)
        if len(chunk_text) >= MIN_CHUNK_SIZE:
            title = chunk_text[:50] + "..." if len(chunk_text) > 50 else chunk_text

            chunk = {
                "title": title,
                "chapter": None,
                "content": chunk_text,
                "metadata": {
                    "category": file_name,
                    "source": file_path,
                    "page_number": None,
                    "chunk_type": "semantic"
                }
            }
            chunks.append(chunk)

    return chunks


# ========== 8-1. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ê¸°ë°˜ ì²­í‚¹ (ê°œì„ ) ==========

class MarkdownBlock:
    """ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ì„ í‘œí˜„í•˜ëŠ” í´ë˜ìŠ¤"""
    def __init__(self, block_type, content, level=0):
        self.block_type = block_type  # heading, paragraph, table, list, code, blockquote
        self.content = content
        self.level = level  # í—¤ë”© ë ˆë²¨ (1-6) ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ ê¹Šì´

    def __len__(self):
        return len(self.content)

    def is_atomic(self):
        """ë¶„í• í•˜ë©´ ì•ˆ ë˜ëŠ” ë¸”ë¡ì¸ì§€ í™•ì¸"""
        return self.block_type in ('table', 'code', 'blockquote')


def parse_markdown_blocks(markdown_text):
    """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ ë¸”ë¡ ë‹¨ìœ„ë¡œ íŒŒì‹±"""
    blocks = []
    lines = markdown_text.split('\n')

    i = 0
    while i < len(lines):
        line = lines[i]

        # ë¹ˆ ì¤„ ìŠ¤í‚µ
        if not line.strip():
            i += 1
            continue

        # 1. ì½”ë“œ ë¸”ë¡ (```)
        if line.strip().startswith('```'):
            code_lines = [line]
            i += 1
            while i < len(lines) and not lines[i].strip().startswith('```'):
                code_lines.append(lines[i])
                i += 1
            if i < len(lines):
                code_lines.append(lines[i])  # ë‹«ëŠ” ```
                i += 1
            blocks.append(MarkdownBlock('code', '\n'.join(code_lines)))
            continue

        # 2. í…Œì´ë¸” (| ë¡œ ì‹œì‘)
        if line.strip().startswith('|') or (i + 1 < len(lines) and '|---' in lines[i + 1]):
            table_lines = []
            while i < len(lines) and (lines[i].strip().startswith('|') or '|---' in lines[i] or lines[i].strip().endswith('|')):
                table_lines.append(lines[i])
                i += 1
            if table_lines:
                blocks.append(MarkdownBlock('table', '\n'.join(table_lines)))
            continue

        # 3. í—¤ë”© (# ~ ######)
        heading_match = re.match(r'^(#{1,6})\s+(.+)$', line)
        if heading_match:
            level = len(heading_match.group(1))
            blocks.append(MarkdownBlock('heading', line, level=level))
            i += 1
            continue

        # 4. ì¸ìš© ë¸”ë¡ (>)
        if line.strip().startswith('>'):
            quote_lines = []
            while i < len(lines) and (lines[i].strip().startswith('>') or (lines[i].strip() and quote_lines)):
                if lines[i].strip().startswith('>'):
                    quote_lines.append(lines[i])
                    i += 1
                elif lines[i].strip():  # ì—°ì†ëœ ì¸ìš© ë‚´ìš©
                    quote_lines.append(lines[i])
                    i += 1
                else:
                    break
            blocks.append(MarkdownBlock('blockquote', '\n'.join(quote_lines)))
            continue

        # 5. ë¦¬ìŠ¤íŠ¸ (-, *, ìˆ«ì.)
        list_match = re.match(r'^(\s*)([-*]|\d+\.)\s+', line)
        if list_match:
            list_lines = []
            base_indent = len(list_match.group(1))
            while i < len(lines):
                current_line = lines[i]
                # ë¦¬ìŠ¤íŠ¸ í•­ëª©ì´ê±°ë‚˜ ë“¤ì—¬ì“°ê¸°ëœ ì—°ì† ë‚´ìš©
                is_list_item = re.match(r'^(\s*)([-*]|\d+\.)\s+', current_line)
                is_continuation = current_line.startswith(' ' * (base_indent + 2)) and current_line.strip()

                if is_list_item or is_continuation:
                    list_lines.append(current_line)
                    i += 1
                elif not current_line.strip():  # ë¹ˆ ì¤„ì€ ë¦¬ìŠ¤íŠ¸ ëì¼ ìˆ˜ ìˆìŒ
                    # ë‹¤ìŒ ì¤„ì´ ë¦¬ìŠ¤íŠ¸ë©´ ê³„ì†
                    if i + 1 < len(lines) and re.match(r'^(\s*)([-*]|\d+\.)\s+', lines[i + 1]):
                        list_lines.append(current_line)
                        i += 1
                    else:
                        break
                else:
                    break

            if list_lines:
                blocks.append(MarkdownBlock('list', '\n'.join(list_lines)))
            continue

        # 6. ì¼ë°˜ ë¬¸ë‹¨
        para_lines = [line]
        i += 1
        while i < len(lines):
            next_line = lines[i]
            # ë¹ˆ ì¤„ì´ë©´ ë¬¸ë‹¨ ë
            if not next_line.strip():
                i += 1
                break
            # ë‹¤ë¥¸ ë¸”ë¡ ì‹œì‘ì´ë©´ ë¬¸ë‹¨ ë
            if (next_line.strip().startswith('#') or
                next_line.strip().startswith('|') or
                next_line.strip().startswith('>') or
                next_line.strip().startswith('```') or
                re.match(r'^(\s*)([-*]|\d+\.)\s+', next_line)):
                break
            para_lines.append(next_line)
            i += 1

        blocks.append(MarkdownBlock('paragraph', '\n'.join(para_lines)))

    return blocks


def get_heading_context(blocks, current_index):
    """í˜„ì¬ ìœ„ì¹˜ì˜ ìƒìœ„ í—¤ë”© ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜"""
    context = []
    current_level = 7  # ìµœëŒ€ ë ˆë²¨ë³´ë‹¤ ë†’ê²Œ ì‹œì‘

    for i in range(current_index - 1, -1, -1):
        block = blocks[i]
        if block.block_type == 'heading' and block.level < current_level:
            context.insert(0, block.content)
            current_level = block.level
            if current_level == 1:
                break

    return context


def chunk_markdown_semantic(markdown_text, file_name, file_path):
    """ë§ˆí¬ë‹¤ìš´ ë¬¸ë²•ì„ ë³´ì¡´í•˜ë©´ì„œ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹"""
    chunks = []

    if not markdown_text.strip():
        return chunks

    # ë§ˆí¬ë‹¤ìš´ì„ ë¸”ë¡ ë‹¨ìœ„ë¡œ íŒŒì‹±
    blocks = parse_markdown_blocks(markdown_text)

    if not blocks:
        return chunks

    print(f"   ğŸ“‘ íŒŒì‹±ëœ ë¸”ë¡: {len(blocks)}ê°œ")
    block_types = {}
    for b in blocks:
        block_types[b.block_type] = block_types.get(b.block_type, 0) + 1
    print(f"      ë¸”ë¡ íƒ€ì…: {block_types}")

    # ë¸”ë¡ë“¤ì„ ì²­í¬ë¡œ ê·¸ë£¹í™”
    current_chunk_blocks = []
    current_length = 0
    current_heading = file_name
    heading_context = []  # ìƒìœ„ í—¤ë”© ì»¨í…ìŠ¤íŠ¸

    for i, block in enumerate(blocks):
        block_length = len(block)

        # í—¤ë”© ë¸”ë¡ ì²˜ë¦¬
        if block.block_type == 'heading':
            # ì´ì „ ì²­í¬ ì €ì¥ (ë‚´ìš©ì´ ìˆìœ¼ë©´)
            if current_chunk_blocks and any(b.block_type != 'heading' for b in current_chunk_blocks):
                chunk_content = build_chunk_content(current_chunk_blocks, heading_context)
                if len(chunk_content) >= MIN_CHUNK_SIZE:
                    chunks.append(create_chunk(current_heading, chunk_content, file_name, file_path, "markdown_heading"))

            # ìƒˆ ì²­í¬ ì‹œì‘
            current_chunk_blocks = [block]
            current_length = block_length
            current_heading = re.sub(r'^#+\s*', '', block.content).strip()
            heading_context = get_heading_context(blocks, i)
            continue

        # ì›ìì  ë¸”ë¡(í…Œì´ë¸”, ì½”ë“œë¸”ë¡)ì´ ë„ˆë¬´ í¬ë©´ ë‹¨ë… ì²­í¬ë¡œ
        if block.is_atomic() and block_length > MAX_CHUNK_SIZE:
            # ì´ì „ ì²­í¬ ì €ì¥
            if current_chunk_blocks:
                chunk_content = build_chunk_content(current_chunk_blocks, heading_context)
                if len(chunk_content) >= MIN_CHUNK_SIZE:
                    chunks.append(create_chunk(current_heading, chunk_content, file_name, file_path, "markdown_heading"))
                current_chunk_blocks = []
                current_length = 0

            # í° ì›ìì  ë¸”ë¡ì€ ë¶„í• í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ì €ì¥
            chunk_content = block.content
            if heading_context:
                chunk_content = '\n'.join(heading_context) + '\n\n' + chunk_content
            chunks.append(create_chunk(current_heading, chunk_content, file_name, file_path, f"markdown_{block.block_type}"))
            continue

        # í˜„ì¬ ì²­í¬ì— ë¸”ë¡ ì¶”ê°€ ì‹œ í¬ê¸° ì´ˆê³¼ ì²´í¬
        if current_length + block_length > MAX_CHUNK_SIZE and current_chunk_blocks:
            # ì›ìì  ë¸”ë¡ì€ ë¶„ë¦¬í•´ì„œ ë‹¤ìŒ ì²­í¬ë¡œ
            if block.is_atomic():
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunk_content = build_chunk_content(current_chunk_blocks, heading_context)
                if len(chunk_content) >= MIN_CHUNK_SIZE:
                    chunks.append(create_chunk(current_heading, chunk_content, file_name, file_path, "markdown_semantic"))

                # ìƒˆ ì²­í¬ ì‹œì‘ (ì›ìì  ë¸”ë¡ìœ¼ë¡œ)
                current_chunk_blocks = [block]
                current_length = block_length
            else:
                # ì¼ë°˜ ë¸”ë¡ë„ í˜„ì¬ ì²­í¬ ì €ì¥ í›„ ìƒˆ ì²­í¬ ì‹œì‘
                chunk_content = build_chunk_content(current_chunk_blocks, heading_context)
                if len(chunk_content) >= MIN_CHUNK_SIZE:
                    chunks.append(create_chunk(current_heading, chunk_content, file_name, file_path, "markdown_semantic"))

                current_chunk_blocks = [block]
                current_length = block_length
        else:
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€
            current_chunk_blocks.append(block)
            current_length += block_length

    # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
    if current_chunk_blocks:
        chunk_content = build_chunk_content(current_chunk_blocks, heading_context)
        if len(chunk_content) >= MIN_CHUNK_SIZE:
            chunks.append(create_chunk(current_heading, chunk_content, file_name, file_path, "markdown_semantic"))

    return chunks


def build_chunk_content(blocks, heading_context=None):
    """ë¸”ë¡ë“¤ì„ í•˜ë‚˜ì˜ ì²­í¬ ì½˜í…ì¸ ë¡œ ì¡°í•©"""
    parts = []

    # ìƒìœ„ í—¤ë”© ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì˜µì…˜)
    if heading_context:
        for ctx in heading_context:
            parts.append(ctx)
        parts.append('')  # ë¹ˆ ì¤„ êµ¬ë¶„

    for block in blocks:
        parts.append(block.content)

    return '\n\n'.join(parts)


def create_chunk(title, content, file_name, file_path, chunk_type):
    """ì²­í¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±"""
    # ì œëª©ì—ì„œ ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±°
    clean_title = re.sub(r'^#+\s*', '', title).strip()
    if len(clean_title) > 50:
        clean_title = clean_title[:50] + "..."

    return {
        "title": clean_title,
        "chapter": None,
        "content": content,
        "metadata": {
            "category": file_name,
            "source": file_path,
            "page_number": None,
            "chunk_type": chunk_type
        }
    }


def split_large_section(section, file_name, file_path):
    """í° ì„¹ì…˜ì„ ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë¶„í• """
    chunks = []
    content = section["content"]
    title = section["title"]

    # ë¸”ë¡ ë‹¨ìœ„ë¡œ íŒŒì‹±
    blocks = parse_markdown_blocks(content)

    current_chunk_blocks = []
    current_length = 0
    part_num = 1

    for block in blocks:
        block_length = len(block)

        # ì›ìì  ë¸”ë¡ì´ ë„ˆë¬´ í¬ë©´ ë‹¨ë… ì²­í¬ë¡œ
        if block.is_atomic() and block_length > MAX_CHUNK_SIZE:
            # ì´ì „ ì²­í¬ ì €ì¥
            if current_chunk_blocks:
                chunk_content = build_chunk_content(current_chunk_blocks)
                chunks.append({
                    "title": f"{title} (Part {part_num})",
                    "chapter": None,
                    "content": chunk_content,
                    "metadata": {
                        "category": file_name,
                        "source": file_path,
                        "page_number": None,
                        "chunk_type": "markdown_semantic"
                    }
                })
                part_num += 1
                current_chunk_blocks = []
                current_length = 0

            # í° ë¸”ë¡ ë‹¨ë… ì €ì¥
            chunks.append({
                "title": f"{title} (Part {part_num})",
                "chapter": None,
                "content": block.content,
                "metadata": {
                    "category": file_name,
                    "source": file_path,
                    "page_number": None,
                    "chunk_type": f"markdown_{block.block_type}"
                }
            })
            part_num += 1
            continue

        if current_length + block_length > MAX_CHUNK_SIZE and current_chunk_blocks:
            chunk_content = build_chunk_content(current_chunk_blocks)
            chunks.append({
                "title": f"{title} (Part {part_num})",
                "chapter": None,
                "content": chunk_content,
                "metadata": {
                    "category": file_name,
                    "source": file_path,
                    "page_number": None,
                    "chunk_type": "markdown_semantic"
                }
            })
            part_num += 1
            current_chunk_blocks = [block]
            current_length = block_length
        else:
            current_chunk_blocks.append(block)
            current_length += block_length

    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk_blocks:
        chunk_content = build_chunk_content(current_chunk_blocks)
        if len(chunk_content) >= MIN_CHUNK_SIZE:
            chunks.append({
                "title": f"{title} (Part {part_num})" if part_num > 1 else title,
                "chapter": None,
                "content": chunk_content,
                "metadata": {
                    "category": file_name,
                    "source": file_path,
                    "page_number": None,
                    "chunk_type": "markdown_semantic"
                }
            })

    return chunks


def chunk_markdown_by_paragraph(markdown_text, file_name, file_path):
    """í—¤ë”©ì´ ì—†ëŠ” ê²½ìš° ë¸”ë¡ ê¸°ë°˜ ì²­í‚¹"""
    # ë¸”ë¡ ë‹¨ìœ„ë¡œ íŒŒì‹±í•˜ì—¬ ì²˜ë¦¬
    blocks = parse_markdown_blocks(markdown_text)

    if not blocks:
        return []

    chunks = []
    current_chunk_blocks = []
    current_length = 0

    for block in blocks:
        block_length = len(block)

        # ì›ìì  ë¸”ë¡ì´ ë„ˆë¬´ í¬ë©´ ë‹¨ë… ì²­í¬ë¡œ
        if block.is_atomic() and block_length > MAX_CHUNK_SIZE:
            if current_chunk_blocks:
                chunk_content = build_chunk_content(current_chunk_blocks)
                title = get_title_from_content(chunk_content)
                chunks.append(create_chunk(title, chunk_content, file_name, file_path, "markdown_paragraph"))
                current_chunk_blocks = []
                current_length = 0

            title = f"{block.block_type.capitalize()} block"
            chunks.append(create_chunk(title, block.content, file_name, file_path, f"markdown_{block.block_type}"))
            continue

        if current_length + block_length > MAX_CHUNK_SIZE and current_chunk_blocks:
            chunk_content = build_chunk_content(current_chunk_blocks)
            title = get_title_from_content(chunk_content)
            chunks.append(create_chunk(title, chunk_content, file_name, file_path, "markdown_paragraph"))

            current_chunk_blocks = [block]
            current_length = block_length
        else:
            current_chunk_blocks.append(block)
            current_length += block_length

    # ë§ˆì§€ë§‰ ì²­í¬
    if current_chunk_blocks:
        chunk_content = build_chunk_content(current_chunk_blocks)
        if len(chunk_content) >= MIN_CHUNK_SIZE:
            title = get_title_from_content(chunk_content)
            chunks.append(create_chunk(title, chunk_content, file_name, file_path, "markdown_paragraph"))

    return chunks


def get_title_from_content(content):
    """ì½˜í…ì¸ ì—ì„œ ì œëª© ì¶”ì¶œ"""
    # ì²« ì¤„ì—ì„œ ë§ˆí¬ë‹¤ìš´ ê¸°í˜¸ ì œê±°
    first_line = content.split('\n')[0].strip()
    title = re.sub(r'^#+\s*', '', first_line)
    title = re.sub(r'\*+', '', title)
    title = re.sub(r'^\|.*\|$', 'Table', title)  # í…Œì´ë¸”ì´ë©´ Tableë¡œ

    if len(title) > 50:
        title = title[:50] + "..."

    return title if title else "Untitled"


# ========== 9. í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ (ë©”ì¸) ==========
def chunk_hybrid(elements, file_name, file_path, use_markdown=True):
    """ë¬¸ì„œ êµ¬ì¡°ì— ë”°ë¼ ì í•©í•œ ì²­í‚¹ ë°©ì‹ ìë™ ì„ íƒ

    Args:
        elements: Unstructuredì—ì„œ ì¶”ì¶œí•œ elements
        file_name: íŒŒì¼ëª…
        file_path: íŒŒì¼ ê²½ë¡œ
        use_markdown: ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    """

    if use_markdown:
        # ë§ˆí¬ë‹¤ìš´ ê¸°ë°˜ ì²­í‚¹ (ê¶Œì¥)
        print(f"   ğŸ“ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ëª¨ë“œ ì‚¬ìš©")

        # elementsë¥¼ ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë³€í™˜
        markdown_text = convert_elements_to_markdown(elements, file_name)

        if not markdown_text.strip():
            print(f"   âš ï¸ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŒ, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì „í™˜")
            return chunk_hybrid_legacy(elements, file_name, file_path)

        # ë¶€ì¹™ í•„í„°ë§: ì—¬ëŸ¬ ë¶€ì¹™ ì¤‘ ë§ˆì§€ë§‰ ê²ƒë§Œ ìœ ì§€
        markdown_text = filter_appendix_sections(markdown_text)

        # ë§ˆí¬ë‹¤ìš´ ê¸°ë°˜ ì‹œë§¨í‹± ì²­í‚¹
        chunks = chunk_markdown_semantic(markdown_text, file_name, file_path)

        if not chunks:
            print(f"   âš ï¸ ë§ˆí¬ë‹¤ìš´ ì²­í‚¹ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì „í™˜")
            return chunk_hybrid_legacy(elements, file_name, file_path)

        print(f"   âœ… ë§ˆí¬ë‹¤ìš´ ê¸°ë°˜ ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬")
        return chunks

    else:
        # ê¸°ì¡´ ë°©ì‹
        return chunk_hybrid_legacy(elements, file_name, file_path)


def chunk_hybrid_legacy(elements, file_name, file_path):
    """ê¸°ì¡´ í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ ë°©ì‹ (í´ë°±ìš©)"""

    # êµ¬ì¡° ê°ì§€
    structure_type = detect_document_structure(elements)
    print(f"   ğŸ” ê°ì§€ëœ êµ¬ì¡°: {structure_type}")

    # êµ¬ì¡°ì— ë§ëŠ” ì²­í‚¹ ë°©ì‹ ì ìš©
    if structure_type == "article":
        chunks = chunk_by_article(elements, file_name, file_path)
    elif structure_type == "title":
        chunks = chunk_by_title(elements, file_name, file_path)
    elif structure_type == "qa":
        chunks = chunk_by_qa(elements, file_name, file_path)
    elif structure_type == "numbered" or structure_type == "bracket":
        chunks = chunk_by_number(elements, file_name, file_path)
    else:
        chunks = chunk_semantic(elements, file_name, file_path)

    # ì²­í¬ê°€ ì—†ìœ¼ë©´ ì‹œë§¨í‹±ìœ¼ë¡œ í´ë°±
    if not chunks:
        print(f"   âš ï¸ {structure_type} ì²­í‚¹ ì‹¤íŒ¨, ì‹œë§¨í‹± ì²­í‚¹ìœ¼ë¡œ ì „í™˜")
        chunks = chunk_semantic(elements, file_name, file_path)

    return chunks


# ========== 10. í‚¤ì›Œë“œ ì¶”ì¶œ (GPT í™œìš©) ==========
def extract_keywords(text, max_keywords=5):
    """GPTë¥¼ í™œìš©í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ ê³ ê°ì„¼í„° ìƒë‹´ ë¬¸ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ê³ ê°ì´ ê²€ìƒ‰í•  ë•Œ ì‚¬ìš©í•  ë§Œí•œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
- ëª…ì‚¬ ìœ„ì£¼ë¡œ ì¶”ì¶œ
- ë™ì˜ì–´, ìœ ì‚¬ì–´ë„ í¬í•¨ (ì˜ˆ: í•´ì§€ â†’ ì·¨ì†Œ, ëŠê¸°)
- êµ¬ì–´ì²´ í‘œí˜„ë„ í¬í•¨ (ì˜ˆ: í™˜ë¶ˆ â†’ ëˆ ëŒë ¤ë°›ê¸°)

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”: {"keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}"""
                },
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ {max_keywords}ê°œ ì¶”ì¶œí•˜ì„¸ìš”:\n\n{text[:1000]}"
                }
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        return result.get("keywords", [])
    
    except Exception as e:
        print(f"   âš ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []


# ========== 11. ë¶„ë¥˜ ìë™ ìƒì„± (GPT í™œìš©) ==========
def classify_content(text, categories=CLASSIFICATION_CATEGORIES):
    """GPTë¥¼ í™œìš©í•˜ì—¬ ì½˜í…ì¸  ë¶„ë¥˜"""
    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"""ë‹¹ì‹ ì€ ê³ ê°ì„¼í„° ë¬¸ì„œë¥¼ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ë‹¤ìŒ ì¹´í…Œê³ ë¦¬ ì¤‘ ê°€ì¥ ì í•©í•œ ê²ƒìœ¼ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.

ì¹´í…Œê³ ë¦¬: {categories}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”: {{"classification": "ì¹´í…Œê³ ë¦¬ëª…", "confidence": 0.0~1.0}}"""
                },
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”:\n\n{text[:1000]}"
                }
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )
        
        result = json.loads(response.choices[0].message.content)
        return result.get("classification", "ê¸°íƒ€"), result.get("confidence", 0.0)
    
    except Exception as e:
        print(f"   âš ï¸ ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        return "ê¸°íƒ€", 0.0


# ========== 12. í‚¤ì›Œë“œ & ë¶„ë¥˜ ì¶”ê°€ ==========
def enrich_chunks(chunks, extract_keywords_flag=True, classify_flag=True):
    """ì²­í¬ì— í‚¤ì›Œë“œì™€ ë¶„ë¥˜ ì¶”ê°€"""
    for i, chunk in enumerate(chunks):
        full_text = f"{chunk['title']}\n{chunk['content']}"
        
        # í‚¤ì›Œë“œ ì¶”ì¶œ
        if extract_keywords_flag:
            keywords = extract_keywords(full_text)
            chunk["keywords"] = keywords
            print(f"      ì²­í¬ {i+1} í‚¤ì›Œë“œ: {keywords}")
        else:
            chunk["keywords"] = []
        
        # ë¶„ë¥˜
        if classify_flag:
            classification, confidence = classify_content(full_text)
            chunk["classification"] = classification
            chunk["classification_confidence"] = confidence
            print(f"      ì²­í¬ {i+1} ë¶„ë¥˜: {classification} (ì‹ ë¢°ë„: {confidence:.2f})")
        else:
            chunk["classification"] = "ê¸°íƒ€"
            chunk["classification_confidence"] = 0.0
    
    return chunks


# ========== 12-1. Contextual Embedding (ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€) ==========
def generate_contextual_description(chunk_content: str, document_context: str) -> str:
    """
    ì²­í¬ì— ëŒ€í•œ ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸ ì„¤ëª…ì„ ìƒì„±
    - ì „ì²´ ë¬¸ì„œ ë§¥ë½ì—ì„œ ì´ ì²­í¬ê°€ ì–´ë–¤ ë‚´ìš©ì¸ì§€ ì„¤ëª…
    """
    try:
        response = client.chat.completions.create(
            model=CONTEXTUAL_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ ê³ ê°ì„¼í„° ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ì²­í¬(chunk)ê°€ ì „ì²´ ë¬¸ì„œì—ì„œ ì–´ë–¤ ë§¥ë½ê³¼ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ”ì§€ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ì„¸ìš”.

ì¶œë ¥ í˜•ì‹:
- 2-3ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±
- ì´ ì²­í¬ê°€ ë‹¤ë£¨ëŠ” í•µì‹¬ ì£¼ì œì™€ ë¬¸ì„œ ë‚´ ìœ„ì¹˜/ì—­í•  ì„¤ëª…
- ê³ ê°ì´ ì´ ì •ë³´ë¥¼ ì°¾ì„ ë•Œ ì‚¬ìš©í•  ë§Œí•œ ìƒí™© í¬í•¨"""
                },
                {
                    "role": "user",
                    "content": f"""## ë¬¸ì„œ ê°œìš”
{document_context[:1500]}

## ë¶„ì„í•  ì²­í¬
{chunk_content[:2000]}

ì´ ì²­í¬ì˜ ë¬¸ì„œ ë‚´ ë§¥ë½ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
                }
            ],
            temperature=0,
            max_tokens=200
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"   âš ï¸ ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return ""


def generate_hypothetical_queries(chunk_content: str, num_queries: int = 3) -> List[str]:
    """
    HyDE (Hypothetical Document Embeddings) ë°©ì‹
    - ì´ ì²­í¬ë¥¼ ì°¾ì„ ë•Œ ì‚¬ìš©í•  ë§Œí•œ ê°€ìƒì˜ ì§ˆë¬¸ë“¤ì„ ìƒì„±
    """
    try:
        response = client.chat.completions.create(
            model=HYDE_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": f"""ë‹¹ì‹ ì€ ê³ ê°ì„¼í„° ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ì„œ ë‚´ìš©ì„ ë³´ê³ , ê³ ê°ì´ ì´ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•´ ë¬¼ì–´ë³¼ ìˆ˜ ìˆëŠ” ì§ˆë¬¸ {num_queries}ê°œë¥¼ ìƒì„±í•˜ì„¸ìš”.

ê·œì¹™:
1. ì‹¤ì œ ê³ ê°ì´ ì‚¬ìš©í•  ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì§ˆë¬¸
2. êµ¬ì–´ì²´ì™€ ë¬¸ì–´ì²´ í˜¼í•©
3. ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹ ì‚¬ìš© (ì§ì ‘ ì§ˆë¬¸, ìƒí™© ì„¤ëª…, ìš”ì²­ ë“±)
4. ë™ì˜ì–´/ìœ ì‚¬ì–´ í™œìš©

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ: {{"queries": ["ì§ˆë¬¸1", "ì§ˆë¬¸2", "ì§ˆë¬¸3"]}}"""
                },
                {
                    "role": "user",
                    "content": f"ë‹¤ìŒ ë‚´ìš©ì— ëŒ€í•œ ê³ ê° ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”:\n\n{chunk_content[:1500]}"
                }
            ],
            temperature=0.7,
            response_format={"type": "json_object"}
        )
        result = json.loads(response.choices[0].message.content)
        return result.get("queries", [])
    except Exception as e:
        print(f"   âš ï¸ ê°€ìƒ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨: {e}")
        return []


def create_enhanced_embedding_text(
    chunk: Dict,
    document_context: str,
    use_contextual: bool = True,
    use_hyde: bool = True
) -> Tuple[str, Dict]:
    """
    í–¥ìƒëœ ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„± (Contextual + HyDE ê²°í•©)

    Returns:
        Tuple[str, Dict]: (ì„ë² ë”©í•  í…ìŠ¤íŠ¸, ì¶”ê°€ ë©”íƒ€ë°ì´í„°)
    """
    title = chunk.get('title', '')
    content = chunk.get('content', '')
    keywords = chunk.get('keywords', [])
    classification = chunk.get('classification', 'ê¸°íƒ€')

    # ê¸°ë³¸ í…ìŠ¤íŠ¸ êµ¬ì„±
    base_text = f"""[ë¶„ë¥˜: {classification}]
[í‚¤ì›Œë“œ: {', '.join(keywords) if keywords else ''}]
{title}
{content}"""

    enhanced_parts = [base_text]
    extra_metadata = {}

    # 1. Contextual Embedding
    if use_contextual and document_context:
        contextual_desc = generate_contextual_description(content, document_context)
        if contextual_desc:
            enhanced_parts.append(f"\n[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{contextual_desc}")
            extra_metadata['contextual_description'] = contextual_desc

    # 2. HyDE - ê°€ìƒ ì§ˆë¬¸ ì¶”ê°€
    if use_hyde:
        hypothetical_queries = generate_hypothetical_queries(content)
        if hypothetical_queries:
            queries_text = "\n".join([f"- {q}" for q in hypothetical_queries])
            enhanced_parts.append(f"\n[ì˜ˆìƒ ì§ˆë¬¸]\n{queries_text}")
            extra_metadata['hypothetical_queries'] = hypothetical_queries

    enhanced_text = "\n".join(enhanced_parts)

    return enhanced_text, extra_metadata


async def generate_contextual_async(chunk_content: str, document_context: str, semaphore: asyncio.Semaphore) -> str:
    """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
    async with semaphore:
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            generate_contextual_description,
            chunk_content,
            document_context
        )


async def generate_hyde_async(chunk_content: str, semaphore: asyncio.Semaphore) -> List[str]:
    """ë¹„ë™ê¸° HyDE ì§ˆë¬¸ ìƒì„±"""
    async with semaphore:
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            generate_hypothetical_queries,
            chunk_content
        )


async def process_chunks_async(
    chunks: List[Dict],
    document_context: str,
    document_keywords: List[str] = None,
    use_contextual: bool = True,
    use_hyde: bool = True
) -> List[Tuple[str, Dict]]:
    """
    ì²­í¬ë“¤ì„ ë¹„ë™ê¸°ë¡œ ë³‘ë ¬ ì²˜ë¦¬í•˜ì—¬ í–¥ìƒëœ ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„±

    Args:
        chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        document_context: ë¬¸ì„œ ìš”ì•½/ì»¨í…ìŠ¤íŠ¸
        document_keywords: ë¬¸ì„œ ì „ì²´ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        use_contextual: Contextual Embedding ì‚¬ìš© ì—¬ë¶€
        use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
    """
    semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
    results = []

    print(f"   ğŸš€ {len(chunks)}ê°œ ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
    start_time = time.time()

    # ë¬¸ì„œ í‚¤ì›Œë“œ ë¬¸ìì—´ ì¤€ë¹„
    doc_keywords_str = ', '.join(document_keywords) if document_keywords else ''

    async def process_single_chunk(chunk: Dict, idx: int) -> Tuple[str, Dict]:
        title = chunk.get('title', '')
        content = chunk.get('content', '')
        chunk_keywords = chunk.get('keywords', [])
        classification = chunk.get('classification', 'ê¸°íƒ€')

        # ì²­í¬ í‚¤ì›Œë“œ + ë¬¸ì„œ í‚¤ì›Œë“œ ê²°í•© (ì¤‘ë³µ ì œê±°)
        all_keywords = list(chunk_keywords) if chunk_keywords else []
        if document_keywords:
            for kw in document_keywords:
                if kw not in all_keywords:
                    all_keywords.append(kw)

        combined_keywords_str = ', '.join(all_keywords) if all_keywords else ''

        # ê¸°ë³¸ í…ìŠ¤íŠ¸ (ë¬¸ì„œ í‚¤ì›Œë“œ í¬í•¨)
        base_text = f"""[ë¶„ë¥˜: {classification}]
[ë¬¸ì„œ í‚¤ì›Œë“œ: {doc_keywords_str}]
[ì²­í¬ í‚¤ì›Œë“œ: {', '.join(chunk_keywords) if chunk_keywords else ''}]
{title}
{content}"""

        enhanced_parts = [base_text]
        extra_metadata = {
            'document_keywords': document_keywords or [],
            'combined_keywords': all_keywords
        }

        # ë³‘ë ¬ë¡œ Contextualê³¼ HyDE ì²˜ë¦¬
        tasks = []
        if use_contextual and document_context:
            tasks.append(('contextual', generate_contextual_async(content, document_context, semaphore)))
        if use_hyde:
            tasks.append(('hyde', generate_hyde_async(content, semaphore)))

        if tasks:
            task_results = await asyncio.gather(*[t[1] for t in tasks], return_exceptions=True)

            for i, (task_type, _) in enumerate(tasks):
                result = task_results[i]
                if isinstance(result, Exception):
                    print(f"      âš ï¸ ì²­í¬ {idx+1} {task_type} ì‹¤íŒ¨: {result}")
                    continue

                if task_type == 'contextual' and result:
                    enhanced_parts.append(f"\n[ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸]\n{result}")
                    extra_metadata['contextual_description'] = result
                elif task_type == 'hyde' and result:
                    queries_text = "\n".join([f"- {q}" for q in result])
                    enhanced_parts.append(f"\n[ì˜ˆìƒ ì§ˆë¬¸]\n{queries_text}")
                    extra_metadata['hypothetical_queries'] = result

        return "\n".join(enhanced_parts), extra_metadata

    # ëª¨ë“  ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬
    tasks = [process_single_chunk(chunk, i) for i, chunk in enumerate(chunks)]
    results = await asyncio.gather(*tasks)

    elapsed = time.time() - start_time
    print(f"   âœ… ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ: {elapsed:.2f}ì´ˆ ({len(chunks)/elapsed:.1f} ì²­í¬/ì´ˆ)")

    return results


def enrich_chunks_with_embeddings(
    chunks: List[Dict],
    document_context: str = "",
    document_keywords: List[str] = None,
    use_contextual: bool = True,
    use_hyde: bool = True
) -> List[Dict]:
    """
    ì²­í¬ë“¤ì— í–¥ìƒëœ ì„ë² ë”© ì •ë³´ ì¶”ê°€ (ë™ê¸° ë˜í¼)

    Args:
        chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        document_context: ë¬¸ì„œ ìš”ì•½/ì»¨í…ìŠ¤íŠ¸
        document_keywords: ë¬¸ì„œ ì „ì²´ í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        use_contextual: Contextual Embedding ì‚¬ìš© ì—¬ë¶€
        use_hyde: HyDE ì‚¬ìš© ì—¬ë¶€
    """
    try:
        # ì´ë²¤íŠ¸ ë£¨í”„ ì‹¤í–‰
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        results = loop.run_until_complete(
            process_chunks_async(
                chunks,
                document_context,
                document_keywords=document_keywords,
                use_contextual=use_contextual,
                use_hyde=use_hyde
            )
        )

        # ê²°ê³¼ë¥¼ ì²­í¬ì— ë°˜ì˜
        for i, (enhanced_text, extra_metadata) in enumerate(results):
            chunks[i]['enhanced_text'] = enhanced_text
            chunks[i]['extra_metadata'] = extra_metadata

        return chunks

    except Exception as e:
        print(f"   âŒ í–¥ìƒëœ ì„ë² ë”© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš© (ë¬¸ì„œ í‚¤ì›Œë“œ í¬í•¨)
        doc_keywords_str = ', '.join(document_keywords) if document_keywords else ''

        for chunk in chunks:
            title = chunk.get('title', '')
            content = chunk.get('content', '')
            keywords = chunk.get('keywords', [])
            classification = chunk.get('classification', 'ê¸°íƒ€')

            chunk['enhanced_text'] = f"""[ë¶„ë¥˜: {classification}]
[ë¬¸ì„œ í‚¤ì›Œë“œ: {doc_keywords_str}]
[ì²­í¬ í‚¤ì›Œë“œ: {', '.join(keywords) if keywords else ''}]
{title}
{content}"""
            chunk['extra_metadata'] = {'document_keywords': document_keywords or []}

        return chunks
    finally:
        loop.close()


def get_document_summary(elements, file_name: str) -> str:
    """ë¬¸ì„œ ì „ì²´ ìš”ì•½ ìƒì„± (Contextual Embeddingìš©)"""
    try:
        # ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì•ë¶€ë¶„ ìœ„ì£¼)
        full_text = "\n".join([el.text for el in elements[:30] if hasattr(el, 'text') and el.text])

        response = client.chat.completions.create(
            model=CONTEXTUAL_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": """ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš”.
- ë¬¸ì„œì˜ ì£¼ì œì™€ ëª©ì 
- ì£¼ìš” ë‹¤ë£¨ëŠ” ë‚´ìš©ë“¤
- ëŒ€ìƒ ë…ìë‚˜ ì‚¬ìš© ë§¥ë½

3-5ë¬¸ì¥ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±."""
                },
                {
                    "role": "user",
                    "content": f"ë¬¸ì„œëª…: {file_name}\n\në‚´ìš©:\n{full_text[:3000]}"
                }
            ],
            temperature=0,
            max_tokens=300
        )

        summary = response.choices[0].message.content.strip()
        print(f"   ğŸ“„ ë¬¸ì„œ ìš”ì•½ ìƒì„± ì™„ë£Œ")
        return f"[ë¬¸ì„œ: {file_name}]\n{summary}"

    except Exception as e:
        print(f"   âš ï¸ ë¬¸ì„œ ìš”ì•½ ì‹¤íŒ¨: {e}")
        return f"[ë¬¸ì„œ: {file_name}]"


def extract_document_keywords(elements, file_name: str, max_keywords: int = 15) -> List[str]:
    """
    PDF ë¬¸ì„œ ì „ì²´ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
    - ë¬¸ì„œì˜ ì£¼ìš” ì£¼ì œ, ìš©ì–´, ê°œë…ì„ í¬ê´„ì ìœ¼ë¡œ ì¶”ì¶œ
    - ê° ì²­í¬ì— ë¬¸ì„œ í‚¤ì›Œë“œë¥¼ í¬í•¨ì‹œì¼œ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ
    """
    try:
        # ì „ì²´ í…ìŠ¤íŠ¸ ìˆ˜ì§‘ (ì•, ì¤‘ê°„, ë ë¶€ë¶„ ìƒ˜í”Œë§)
        all_elements = [el for el in elements if hasattr(el, 'text') and el.text]

        # ë¬¸ì„œ ì „ì²´ë¥¼ ê³ ë¥´ê²Œ ìƒ˜í”Œë§
        sample_texts = []
        if len(all_elements) <= 50:
            sample_texts = [el.text for el in all_elements]
        else:
            # ì•ë¶€ë¶„ 20ê°œ, ì¤‘ê°„ 15ê°œ, ëë¶€ë¶„ 15ê°œ
            front = [el.text for el in all_elements[:20]]
            mid_start = len(all_elements) // 2 - 7
            middle = [el.text for el in all_elements[mid_start:mid_start + 15]]
            back = [el.text for el in all_elements[-15:]]
            sample_texts = front + middle + back

        full_text = "\n".join(sample_texts)

        response = client.chat.completions.create(
            model=CONTEXTUAL_MODEL,
            messages=[
                {
                    "role": "system",
                    "content": f"""ë‹¹ì‹ ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ {max_keywords}ê°œ ì¶”ì¶œí•˜ì„¸ìš”.

ì¶”ì¶œ ê·œì¹™:
1. **ì£¼ìš” ì£¼ì œ**: ë¬¸ì„œê°€ ë‹¤ë£¨ëŠ” í•µì‹¬ ì£¼ì œ/ë¶„ì•¼ (ì˜ˆ: ìš”ê¸ˆì œ, ì¸í„°ë„·, ëª¨ë°”ì¼)
2. **í•µì‹¬ ìš©ì–´**: ë¬¸ì„œì—ì„œ ë°˜ë³µë˜ëŠ” ì¤‘ìš” ìš©ì–´ (ì˜ˆ: í•´ì§€, ê°€ì…, ë³€ê²½)
3. **ì„œë¹„ìŠ¤/ìƒí’ˆëª…**: êµ¬ì²´ì ì¸ ì„œë¹„ìŠ¤ë‚˜ ìƒí’ˆ ì´ë¦„
4. **ê³ ê° ê´€ë ¨ í‚¤ì›Œë“œ**: ê³ ê°ì´ ê²€ìƒ‰í•  ë§Œí•œ í‘œí˜„ (ì˜ˆ: í™˜ë¶ˆ, ìœ„ì•½ê¸ˆ, í˜œíƒ)
5. **ë™ì˜ì–´/ìœ ì‚¬ì–´**: ê°™ì€ ì˜ë¯¸ì˜ ë‹¤ë¥¸ í‘œí˜„ë„ í¬í•¨ (ì˜ˆ: í•´ì§€/ì·¨ì†Œ/ëŠê¸°)

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ: {{"document_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}}"""
                },
                {
                    "role": "user",
                    "content": f"ë¬¸ì„œëª…: {file_name}\n\në‚´ìš©:\n{full_text[:4000]}"
                }
            ],
            temperature=0,
            response_format={"type": "json_object"}
        )

        result = json.loads(response.choices[0].message.content)
        keywords = result.get("document_keywords", [])

        print(f"   ğŸ”‘ ë¬¸ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ: {keywords}")
        return keywords

    except Exception as e:
        print(f"   âš ï¸ ë¬¸ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return []


# ========== 13. ChromaDB ì„¤ì • ==========
def setup_chromadb(api_key):
    """ChromaDB ì»¬ë ‰ì…˜ ì„¤ì •"""
    openai_ef = embedding_functions.OpenAIEmbeddingFunction(
        api_key=api_key,
        model_name=EMBEDDING_MODEL  # ì„¤ì •ì—ì„œ ëª¨ë¸ëª… ê°€ì ¸ì˜¤ê¸°
    )
    
    chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)
    
    collection = chroma_client.get_or_create_collection(
        name=COLLECTION_NAME,
        embedding_function=openai_ef,
        metadata={"hnsw:space": "cosine"}
    )
    
    return collection


# ========== 14. ì„ë² ë”© ë° ì €ì¥ ==========
def insert_chunks_to_chroma(chunks, collection, use_enhanced: bool = True):
    """ì²­í¬ë¥¼ ChromaDBì— ì €ì¥ (í–¥ìƒëœ ì„ë² ë”© í…ìŠ¤íŠ¸ ì‚¬ìš©)

    Args:
        chunks: ì²­í¬ ë¦¬ìŠ¤íŠ¸
        collection: ChromaDB ì»¬ë ‰ì…˜
        use_enhanced: í–¥ìƒëœ ì„ë² ë”© í…ìŠ¤íŠ¸ ì‚¬ìš© ì—¬ë¶€ (Contextual + HyDE)
    """
    if not chunks:
        print("   âš ï¸ ì €ì¥í•  ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    documents = []
    metadatas = []
    ids = []
    skipped = 0
    split_count = 0  # ë¶„í• ëœ ì²­í¬ ìˆ˜

    for i, chunk in enumerate(chunks):
        keywords_str = ", ".join(chunk.get("keywords", []))
        classification = chunk.get("classification", "ê¸°íƒ€")

        # í…ìŠ¤íŠ¸ ì •ì œ
        title = clean_text(chunk.get('title', ''))
        content = clean_text(chunk.get('content', ''))

        # ë¹ˆ ì½˜í…ì¸  ìŠ¤í‚µ
        if not content:
            print(f"   âš ï¸ ì²­í¬ {i+1} ìŠ¤í‚µ: ë¹ˆ ì½˜í…ì¸ ")
            skipped += 1
            continue

        # í–¥ìƒëœ ì„ë² ë”© í…ìŠ¤íŠ¸ ì‚¬ìš© (Contextual + HyDE)
        if use_enhanced and 'enhanced_text' in chunk:
            doc_text = clean_text(chunk['enhanced_text'])
        else:
            # ê¸°ë³¸ í…ìŠ¤íŠ¸ êµ¬ì„±
            doc_text = f"""[ë¶„ë¥˜: {classification}]
[í‚¤ì›Œë“œ: {keywords_str}]
{title}
{content}"""

        # í† í° ìˆ˜ ì²´í¬
        token_count = count_tokens(doc_text)

        if token_count > MAX_EMBEDDING_TOKENS:
            # í† í° ì´ˆê³¼ ì‹œ ë¶„í• 
            print(f"   ğŸ“ ì²­í¬ {i+1} í† í° ì´ˆê³¼ ({token_count} > {MAX_EMBEDDING_TOKENS}), ë¶„í•  ì§„í–‰")

            # í—¤ë” (ë¶„ë¥˜, í‚¤ì›Œë“œ, ì œëª©) í† í° ê³„ì‚°
            header = f"""[ë¶„ë¥˜: {classification}]
[í‚¤ì›Œë“œ: {keywords_str}]
{title}
"""
            header_tokens = count_tokens(header)

            # ì½˜í…ì¸ ë§Œ ë¶„í•  (í—¤ë” í† í° ì œì™¸í•œ í¬ê¸°ë¡œ)
            content_max_tokens = MAX_EMBEDDING_TOKENS - header_tokens - 100  # ì•ˆì „ ë§ˆì§„
            content_chunks = split_text_by_tokens(doc_text, max_tokens=content_max_tokens, overlap_tokens=200)

            print(f"      â†’ {len(content_chunks)}ê°œë¡œ ë¶„í• ë¨")
            split_count += len(content_chunks) - 1  # ì›ë˜ 1ê°œì—ì„œ ì¶”ê°€ëœ ìˆ˜

            # ê° ë¶„í•  ì²­í¬ ì €ì¥
            for j, content_part in enumerate(content_chunks):
                # ìµœì¢… ë¹ˆ ì²´í¬
                if not content_part.strip():
                    continue

                documents.append(content_part)

                # ì¶”ê°€ ë©”íƒ€ë°ì´í„° í¬í•¨
                extra_meta = chunk.get('extra_metadata', {})
                doc_keywords = extra_meta.get('document_keywords', [])
                combined_keywords = extra_meta.get('combined_keywords', [])

                metadatas.append({
                    "category": chunk["metadata"]["category"],
                    "chapter": chunk.get("chapter") or "",
                    "title": f"{title[:180]} (Part {j+1}/{len(content_chunks)})" if title else f"Part {j+1}/{len(content_chunks)}",
                    "source": chunk["metadata"]["source"],
                    "page_number": chunk["metadata"].get("page_number") or 0,
                    "chunk_type": chunk["metadata"].get("chunk_type", "unknown"),
                    "keywords": keywords_str[:500] if keywords_str else "",
                    "document_keywords": ', '.join(doc_keywords)[:500] if doc_keywords else "",
                    "combined_keywords": ', '.join(combined_keywords)[:500] if combined_keywords else "",
                    "classification": classification,
                    "classification_confidence": chunk.get("classification_confidence", 0.0),
                    "is_split": True,
                    "split_part": j + 1,
                    "split_total": len(content_chunks),
                    "has_contextual": bool(extra_meta.get('contextual_description')),
                    "has_hyde": bool(extra_meta.get('hypothetical_queries')),
                    "hypothetical_queries": json.dumps(extra_meta.get('hypothetical_queries', []), ensure_ascii=False)[:500]
                })

                ids.append(f"{chunk['metadata']['category']}_{i}_part{j}")

        else:
            # í† í° ì œí•œ ì´ë‚´ - ê·¸ëŒ€ë¡œ ì €ì¥
            # ìµœì¢… ë¹ˆ ì²´í¬
            if not doc_text.strip():
                print(f"   âš ï¸ ì²­í¬ {i+1} ìŠ¤í‚µ: ì •ì œ í›„ ë¹ˆ í…ìŠ¤íŠ¸")
                skipped += 1
                continue

            documents.append(doc_text)

            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° í¬í•¨
            extra_meta = chunk.get('extra_metadata', {})
            doc_keywords = extra_meta.get('document_keywords', [])
            combined_keywords = extra_meta.get('combined_keywords', [])

            metadatas.append({
                "category": chunk["metadata"]["category"],
                "chapter": chunk.get("chapter") or "",
                "title": title[:200] if title else "",
                "source": chunk["metadata"]["source"],
                "page_number": chunk["metadata"].get("page_number") or 0,
                "chunk_type": chunk["metadata"].get("chunk_type", "unknown"),
                "keywords": keywords_str[:500] if keywords_str else "",
                "document_keywords": ', '.join(doc_keywords)[:500] if doc_keywords else "",
                "combined_keywords": ', '.join(combined_keywords)[:500] if combined_keywords else "",
                "classification": classification,
                "classification_confidence": chunk.get("classification_confidence", 0.0),
                "is_split": False,
                "split_part": 0,
                "split_total": 1,
                "has_contextual": bool(extra_meta.get('contextual_description')),
                "has_hyde": bool(extra_meta.get('hypothetical_queries')),
                "hypothetical_queries": json.dumps(extra_meta.get('hypothetical_queries', []), ensure_ascii=False)[:500]
            })

            ids.append(f"{chunk['metadata']['category']}_{i}")

    if not documents:
        print("   âš ï¸ ìœ íš¨í•œ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ë°°ì¹˜ ì²˜ë¦¬ (í•œ ë²ˆì— ë„ˆë¬´ ë§ì´ ë³´ë‚´ì§€ ì•Šê¸°)
    batch_size = 100
    for start in range(0, len(documents), batch_size):
        end = min(start + batch_size, len(documents))
        try:
            collection.add(
                documents=documents[start:end],
                metadatas=metadatas[start:end],
                ids=ids[start:end]
            )
        except Exception as e:
            print(f"   âŒ ë°°ì¹˜ {start}-{end} ì €ì¥ ì‹¤íŒ¨: {e}")
            # ê°œë³„ ì €ì¥ ì‹œë„
            for j in range(start, end):
                try:
                    collection.add(
                        documents=[documents[j]],
                        metadatas=[metadatas[j]],
                        ids=[ids[j]]
                    )
                except Exception as e2:
                    print(f"      âŒ ê°œë³„ ì²­í¬ {j} ì €ì¥ ì‹¤íŒ¨: {e2}")

    print(f"   âœ… {len(documents)}ê°œ ì²­í¬ ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ")
    print(f"      (ì›ë³¸: {len(chunks)}ê°œ, ë¶„í•  ì¶”ê°€: {split_count}ê°œ, ìŠ¤í‚µ: {skipped}ê°œ)")
    if use_enhanced:
        contextual_count = sum(1 for c in chunks if c.get('extra_metadata', {}).get('contextual_description'))
        hyde_count = sum(1 for c in chunks if c.get('extra_metadata', {}).get('hypothetical_queries'))
        print(f"      (Contextual: {contextual_count}ê°œ, HyDE: {hyde_count}ê°œ ì ìš©)")


# ========== 15. ë©”ì¸ ì‹¤í–‰ ==========
def process_all_pdfs(
    directory,
    api_key,
    extract_keywords_flag=True,
    classify_flag=True,
    use_markdown=True,
    use_contextual=True, #ë¬¸ë§¥ ì„ë² ë”©
    use_hyde=True        #HyDE 
):
    """ë””ë ‰í„°ë¦¬ ë‚´ ëª¨ë“  PDF ì²˜ë¦¬

    Args:
        directory: PDF íŒŒì¼ì´ ìˆëŠ” ë””ë ‰í„°ë¦¬ ê²½ë¡œ
        api_key: OpenAI API í‚¤
        extract_keywords_flag: í‚¤ì›Œë“œ ì¶”ì¶œ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        classify_flag: ë¶„ë¥˜ ìˆ˜í–‰ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        use_markdown: ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True, ê¶Œì¥)
        use_contextual: Contextual Embedding ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
        use_hyde: HyDE(ê°€ìƒ ì§ˆë¬¸ ìƒì„±) ì‚¬ìš© ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    """

    pdf_files = get_pdf_files(directory)

    if not pdf_files:
        print("âŒ PDF íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    collection = setup_chromadb(api_key)

    total_chunks = 0
    success_files = 0
    failed_files = []
    table_count = 0  # ì²˜ë¦¬ëœ í…Œì´ë¸” ìˆ˜

    print(f"\n{'='*50}")
    print(f"ğŸ“‹ ì²˜ë¦¬ ì„¤ì •:")
    print(f"   - ë§ˆí¬ë‹¤ìš´ ë³€í™˜: {'ì‚¬ìš©' if use_markdown else 'ë¯¸ì‚¬ìš©'}")
    print(f"   - í‚¤ì›Œë“œ ì¶”ì¶œ: {'ì‚¬ìš©' if extract_keywords_flag else 'ë¯¸ì‚¬ìš©'}")
    print(f"   - ìë™ ë¶„ë¥˜: {'ì‚¬ìš©' if classify_flag else 'ë¯¸ì‚¬ìš©'}")
    print(f"   - Contextual Embedding: {'ì‚¬ìš©' if use_contextual else 'ë¯¸ì‚¬ìš©'}")
    print(f"   - HyDE (ê°€ìƒ ì§ˆë¬¸): {'ì‚¬ìš©' if use_hyde else 'ë¯¸ì‚¬ìš©'}")
    print(f"{'='*50}\n")

    for file_path in pdf_files:
        file_name = os.path.splitext(os.path.basename(file_path))[0]

        try:
            # ì¶”ì¶œ
            elements = extract_elements(file_path)

            if not elements:
                print(f"   âš ï¸ ì¶”ì¶œëœ ìš”ì†Œ ì—†ìŒ")
                failed_files.append(file_path)
                continue

            # ë¬¸ì„œ ìš”ì•½ ìƒì„± (Contextual Embeddingìš©)
            document_context = ""
            if use_contextual:
                document_context = get_document_summary(elements, file_name)

            # ë¬¸ì„œ ì „ì²´ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
            document_keywords = []
            if use_contextual or use_hyde:
                document_keywords = extract_document_keywords(elements, file_name)

            # í…Œì´ë¸” ìˆ˜ ì¹´ìš´íŠ¸
            file_table_count = sum(1 for el in elements if getattr(el, 'category', '') == 'Table')
            if file_table_count > 0:
                print(f"   ğŸ“Š í…Œì´ë¸” {file_table_count}ê°œ ë°œê²¬ (GPT ë³€í™˜ ì˜ˆì •)")
            table_count += file_table_count

            # í•˜ì´ë¸Œë¦¬ë“œ ì²­í‚¹ (ë§ˆí¬ë‹¤ìš´ ì˜µì…˜ ì „ë‹¬)
            chunks = chunk_hybrid(elements, file_name, file_path, use_markdown=use_markdown)
            print(f"   ğŸ“ {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

            if not chunks:
                failed_files.append(file_path)
                continue

            # í‚¤ì›Œë“œ & ë¶„ë¥˜ ì¶”ê°€
            chunks = enrich_chunks(chunks, extract_keywords_flag, classify_flag)

            # í–¥ìƒëœ ì„ë² ë”© ì²˜ë¦¬ (Contextual + HyDE + ë¬¸ì„œ í‚¤ì›Œë“œ)
            if use_contextual or use_hyde:
                print(f"   ğŸ”„ í–¥ìƒëœ ì„ë² ë”© í…ìŠ¤íŠ¸ ìƒì„± ì¤‘...")
                chunks = enrich_chunks_with_embeddings(
                    chunks,
                    document_context=document_context,
                    document_keywords=document_keywords,
                    use_contextual=use_contextual,
                    use_hyde=use_hyde
                )

            # ì €ì¥
            use_enhanced = use_contextual or use_hyde
            insert_chunks_to_chroma(chunks, collection, use_enhanced=use_enhanced)
            total_chunks += len(chunks)
            success_files += 1

        except Exception as e:
            print(f"   âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            failed_files.append(file_path)

    print(f"\n{'='*50}")
    print(f"ğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"ğŸ“„ ì„±ê³µí•œ PDF: {success_files}ê°œ")
    print(f"âŒ ì‹¤íŒ¨í•œ PDF: {len(failed_files)}ê°œ")
    if failed_files:
        for f in failed_files:
            print(f"   - {f}")
    print(f"ğŸ“Š ì´ ì²­í¬ ìˆ˜: {total_chunks}ê°œ")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ í…Œì´ë¸” ìˆ˜: {table_count}ê°œ")
    print(f"ğŸ’¾ ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {collection.count()}ê°œ")


# ========== ì‹¤í–‰ ==========
if __name__ == "__main__":
    process_all_pdfs(
        directory=PDF_DIRECTORY,
        api_key=OPENAI_API_KEY,
        extract_keywords_flag=True,
        classify_flag=True,
        use_markdown=True,       # ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‚¬ìš© (í…Œì´ë¸” GPT ë³€í™˜ í¬í•¨)
        use_contextual=True,     # Contextual Embedding (ë¬¸ì„œ ë§¥ë½ ì¶”ê°€)
        use_hyde=True            # HyDE (ê°€ìƒ ì§ˆë¬¸ ìƒì„±)
    )