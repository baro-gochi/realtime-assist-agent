"""Google Cloud Speech-to-Text v2 ÏÑúÎπÑÏä§ ÌÜµÌï© Î™®Îìà.

Ïù¥ Î™®ÎìàÏùÄ Google Cloud Speech-to-Text API v2Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ïã§ÏãúÍ∞Ñ Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶ºÏùÑ
ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôòÌïòÎäî Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.

Ï£ºÏöî Í∏∞Îä•:
    - Ïã§ÏãúÍ∞Ñ Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º Ïù∏Ïãù (Streaming Recognition)
    - WebRTC Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑÏùÑ Google STT API ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
    - ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨Î•º ÌÜµÌïú ÎÜíÏùÄ Ï≤òÎ¶¨Îüâ
    - ÌïúÍµ≠Ïñ¥ ÏùåÏÑ± Ïù∏Ïãù ÏµúÏ†ÅÌôî

Architecture:
    - Google Cloud Speech-to-Text API v2 ÏÇ¨Ïö©
    - Recognizer Í∏∞Î∞ò Ïä§Ìä∏Î¶¨Î∞ç Ïù∏Ïãù
    - AudioFrame ‚Üí PCM bytes Î≥ÄÌôò ÌååÏù¥ÌîÑÎùºÏù∏
    - ÏûêÎèô Íµ¨ÎëêÏ†ê Î∞è Ïã§ÏãúÍ∞Ñ Í≤∞Í≥º ÏßÄÏõê

Examples:
    Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï:
        >>> service = STTService()
        >>> async for text in service.process_audio_stream(audio_frames):
        ...     print(f"Ïù∏ÏãùÎêú ÌÖçÏä§Ìä∏: {text}")

    Ïª§Ïä§ÌÖÄ ÏÑ§Ï†ï:
        >>> service = STTService(
        ...     language_codes=["ko-KR"],
        ...     model="chirp",
        ...     enable_automatic_punctuation=True
        ... )

See Also:
    peer_manager.py: Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑ Ï∫°Ï≤ò
    app.py: WebSocketÏùÑ ÌÜµÌïú Í≤∞Í≥º Ï†ÑÏÜ°
    Google Cloud Speech-to-Text V2 Documentation:
        https://cloud.google.com/speech-to-text/v2/docs
"""
import asyncio
import logging
import os
from typing import AsyncIterator, Optional, List
from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech
from av import AudioFrame
import numpy as np
import queue
import threading

logger = logging.getLogger(__name__)


class STTService:
    """Google Cloud Speech-to-Text v2 ÏÑúÎπÑÏä§ ÎûòÌçº ÌÅ¥ÎûòÏä§.

    WebRTC Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶ºÏùÑ Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.
    v2 APIÏùò Recognizer Í∏∞Î∞ò Ïä§Ìä∏Î¶¨Î∞ç Ïù∏ÏãùÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.

    Attributes:
        client (SpeechClient): Google Cloud Speech v2 ÎèôÍ∏∞ API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏
        project_id (str): Google Cloud ÌîÑÎ°úÏ†ùÌä∏ ID
        recognizer (str): Recognizer Î¶¨ÏÜåÏä§ Í≤ΩÎ°ú
        language_codes (List[str]): ÏùåÏÑ± Ïù∏Ïãù Ïñ∏Ïñ¥ ÏΩîÎìú Î¶¨Ïä§Ìä∏
        model (str): ÏÇ¨Ïö©Ìï† ÏùåÏÑ± Ïù∏Ïãù Î™®Îç∏
        enable_automatic_punctuation (bool): ÏûêÎèô Íµ¨ÎëêÏ†ê Ï∂îÍ∞Ä Ïó¨Î∂Ä
        enable_interim_results (bool): Ï§ëÍ∞Ñ Í≤∞Í≥º ÌôúÏÑ±Ìôî Ïó¨Î∂Ä

    Note:
        - GOOGLE_APPLICATION_CREDENTIALS ÌôòÍ≤Ω Î≥ÄÏàò ÌïÑÏàò
        - GOOGLE_CLOUD_PROJECT ÌôòÍ≤Ω Î≥ÄÏàò ÌïÑÏàò (ÌîÑÎ°úÏ†ùÌä∏ ID)
        - WebRTC Ïò§ÎîîÏò§Îäî ÏûêÎèôÏúºÎ°ú Ïù∏ÏΩîÎî© Í∞êÏßÄÎê®
        - 25KB Ïä§Ìä∏Î¶º Ï†úÌïú Ï£ºÏùò

    Examples:
        >>> service = STTService()
        >>> # Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º Ï≤òÎ¶¨
        >>> async for transcript in service.process_audio_stream(audio_queue):
        ...     print(f"Ïù∏Ïãù Í≤∞Í≥º: {transcript}")
    """

    def __init__(
        self,
        project_id: Optional[str] = None,
        language_codes: Optional[List[str]] = None,
        model: Optional[str] = None,
        enable_automatic_punctuation: Optional[bool] = None,
        enable_interim_results: Optional[bool] = None,
    ):
        """STTService Ï¥àÍ∏∞Ìôî.

        Args:
            project_id (str, optional): Google Cloud ÌîÑÎ°úÏ†ùÌä∏ ID.
                ÌôòÍ≤Ω Î≥ÄÏàò GOOGLE_CLOUD_PROJECT ÎòêÎäî ÌïÑÏàò
            language_codes (List[str], optional): ÏùåÏÑ± Ïù∏Ïãù Ïñ∏Ïñ¥ ÏΩîÎìú Î¶¨Ïä§Ìä∏.
                ÌôòÍ≤Ω Î≥ÄÏàò STT_LANGUAGE_CODE ÎòêÎäî ["ko-KR"] ÏÇ¨Ïö©
            model (str, optional): ÏùåÏÑ± Ïù∏Ïãù Î™®Îç∏.
                ÌôòÍ≤Ω Î≥ÄÏàò STT_MODEL ÎòêÎäî "chirp" ÏÇ¨Ïö©
            enable_automatic_punctuation (bool, optional): ÏûêÎèô Íµ¨ÎëêÏ†ê Ï∂îÍ∞Ä.
                ÌôòÍ≤Ω Î≥ÄÏàò STT_ENABLE_AUTOMATIC_PUNCTUATION ÎòêÎäî True ÏÇ¨Ïö©
            enable_interim_results (bool, optional): Ï§ëÍ∞Ñ Í≤∞Í≥º ÌôúÏÑ±Ìôî.
                ÌôòÍ≤Ω Î≥ÄÏàò STT_ENABLE_INTERIM_RESULTS ÎòêÎäî False ÏÇ¨Ïö©

        Raises:
            ValueError: GOOGLE_CLOUD_PROJECT ÎØ∏ÏÑ§Ï†ï Ïãú

        Note:
            - .env ÌååÏùºÏóêÏÑú ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú ÌïÑÏöî
            - ÏÑúÎπÑÏä§ Í≥ÑÏ†ï ÌÇ§ ÌååÏùº Í∂åÌïú ÌôïÏù∏ ÌïÑÏöî
            - v2 APIÎäî Recognizer Í∞úÎÖê ÌïÑÏàò
        """
        # Google Cloud Ïù∏Ï¶ù Î∞è ÌîÑÎ°úÏ†ùÌä∏ ÌôïÏù∏
        if not os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
            logger.warning(
                "GOOGLE_APPLICATION_CREDENTIALS not set. "
                "STT service may not work properly."
            )

        # ÌîÑÎ°úÏ†ùÌä∏ ID (v2ÏóêÏÑú ÌïÑÏàò)
        self.project_id = project_id or os.getenv("GOOGLE_CLOUD_PROJECT")
        if not self.project_id:
            raise ValueError(
                "GOOGLE_CLOUD_PROJECT environment variable must be set for v2 API"
            )

        # Initialize Google Cloud Speech v2 sync client
        self.client = SpeechClient()

        # Configuration from environment or defaults
        default_language = os.getenv("STT_LANGUAGE_CODE", "ko-KR")
        self.language_codes = language_codes or [default_language]

        self.model = model or os.getenv("STT_MODEL", "short")

        # Recognizer path (v2ÏóêÏÑú ÌïÑÏàò)
        # '_'Îäî Í∏∞Î≥∏ recognizerÎ•º ÏÇ¨Ïö©ÌïúÎã§Îäî ÏùòÎØ∏
        # v2ÏóêÏÑúÎäî global location ÏÇ¨Ïö©
        self.location = "global"
        self.recognizer = f"projects/{self.project_id}/locations/{self.location}/recognizers/_"

        self.enable_automatic_punctuation = (
            enable_automatic_punctuation
            if enable_automatic_punctuation is not None
            else os.getenv("STT_ENABLE_AUTOMATIC_PUNCTUATION", "true").lower() == "true"
        )

        # Only send final results (not interim) for production use
        self.enable_interim_results = (
            enable_interim_results
            if enable_interim_results is not None
            else os.getenv("STT_ENABLE_INTERIM_RESULTS", "false").lower() == "true"
        )

        logger.info(
            f"STT Service v2 initialized: "
            f"project={self.project_id}, "
            f"location={self.location}, "
            f"languages={self.language_codes}, "
            f"model={self.model}, "
            f"punctuation={self.enable_automatic_punctuation}, "
            f"interim={self.enable_interim_results}"
        )

    def _create_streaming_config(self) -> cloud_speech.StreamingRecognitionConfig:
        """Ïä§Ìä∏Î¶¨Î∞ç Ïù∏ÏãùÏùÑ ÏúÑÌïú Google STT v2 ÏÑ§Ï†ï ÏÉùÏÑ±.

        Returns:
            cloud_speech.StreamingRecognitionConfig: Ïä§Ìä∏Î¶¨Î∞ç Ïù∏Ïãù ÏÑ§Ï†ï Í∞ùÏ≤¥

        Note:
            - ExplicitDecodingConfig: WebRTC Ïò§ÎîîÏò§ ÌòïÏãù Î™ÖÏãúÏ†Å ÏßÄÏ†ï
            - language_codes: Îã§Ï§ë Ïñ∏Ïñ¥ ÏßÄÏõê (Î¶¨Ïä§Ìä∏)
            - model: latest_long Îì±
            - interim_results: Ïã§ÏãúÍ∞Ñ Ï§ëÍ∞Ñ Í≤∞Í≥º (False Í∂åÏû• - ÎÇÆÏùÄ ÏßÄÏó∞ÏãúÍ∞Ñ)
        """
        # RecognitionConfig ÏÉùÏÑ± (v2 Î∞©Ïãù - Î™ÖÏãúÏ†Å Ïù∏ÏΩîÎî©)
        recognition_config = cloud_speech.RecognitionConfig(
            explicit_decoding_config=cloud_speech.ExplicitDecodingConfig(
                encoding=cloud_speech.ExplicitDecodingConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=48000,  # WebRTC default
                audio_channel_count=1,  # Mono
            ),
            language_codes=self.language_codes,
            model=self.model,
        )

        # Features ÏÑ§Ï†ï (Íµ¨ÎëêÏ†ê Îì±)
        if self.enable_automatic_punctuation:
            recognition_config.features = cloud_speech.RecognitionFeatures(
                enable_automatic_punctuation=True
            )

        # StreamingRecognitionConfig ÏÉùÏÑ±
        streaming_config = cloud_speech.StreamingRecognitionConfig(
            config=recognition_config,
        )

        # StreamingRecognitionFeatures Ï∂îÍ∞Ä (interim results Îì±)
        if self.enable_interim_results:
            streaming_config.streaming_features = cloud_speech.StreamingRecognitionFeatures(
                interim_results=True
            )

        return streaming_config

    async def _audio_frame_to_bytes(self, frame: AudioFrame) -> bytes:
        """AudioFrameÏùÑ Google STT API ÌòïÏãùÏùò PCM bytesÎ°ú Î≥ÄÌôò.

        WebRTC AudioFrameÏùÑ 16-bit PCM Î∞îÏù¥Ìä∏ Î∞∞Ïó¥Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.

        Args:
            frame (AudioFrame): WebRTC Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑ

        Returns:
            bytes: 16-bit PCM Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞

        Note:
            - AudioFrame.to_ndarray()Î°ú numpy Î∞∞Ïó¥ Ï∂îÏ∂ú
            - int16 ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò (Google STT ÏöîÍµ¨ÏÇ¨Ìï≠)
            - Ïä§ÌÖåÎ†àÏò§Îäî Î™®ÎÖ∏Î°ú Î≥ÄÌôò (Ï±ÑÎÑê ÌèâÍ∑†)
            - ÎÇÆÏùÄ Î≥ºÎ•® ÏûêÎèô Ï¶ùÌè≠
        """
        # Convert AudioFrame to numpy array
        array = frame.to_ndarray()

        # Handle stereo to mono conversion properly
        # First flatten if multi-dimensional
        if array.ndim > 1:
            array = array.flatten()

        # Check if size suggests stereo (2x samples for interleaved L-R-L-R)
        if array.size == frame.samples * 2:
            # Interleaved stereo: reshape to (samples, 2) and average channels
            array = array.reshape(-1, 2).mean(axis=1).astype(array.dtype)

        # Handle WebRTC audio format conversion
        if array.dtype in (np.float32, np.float64):
            # Float format - convert to int16
            array = (array * 32767).astype(np.int16)
        elif array.dtype == np.int16:
            # Apply gain to low volume audio
            max_val = np.abs(array).max()
            if max_val > 0 and max_val < 5000:
                gain = min(6500.0 / max_val, 20.0)
                array = np.clip(array * gain, -32768, 32767).astype(np.int16)

        return array.tobytes()

    async def process_audio_stream(
        self,
        audio_queue: asyncio.Queue
    ) -> AsyncIterator[str]:
        """Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑ ÌÅêÎ•º Ï≤òÎ¶¨ÌïòÏó¨ ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò.

        ÎπÑÎèôÍ∏∞ Ï†úÎÑàÎ†àÏù¥ÌÑ∞Î°ú Ïó∞ÏÜçÏ†ÅÏù∏ ÏùåÏÑ± Ïù∏Ïãù Í≤∞Í≥ºÎ•º Ïä§Ìä∏Î¶¨Î∞çÌï©ÎãàÎã§.

        Args:
            audio_queue (asyncio.Queue): AudioFrame Í∞ùÏ≤¥Î•º Îã¥ÏùÄ ÎπÑÎèôÍ∏∞ ÌÅê

        Yields:
            str: Ïù∏ÏãùÎêú ÌÖçÏä§Ìä∏ (ÏµúÏ¢Ö Í≤∞Í≥ºÎßå ÎòêÎäî Ï§ëÍ∞Ñ Í≤∞Í≥º Ìè¨Ìï®)

        Note:
            - ÌÅêÏóêÏÑú NoneÏùÑ Î∞õÏúºÎ©¥ Ïä§Ìä∏Î¶º Ï¢ÖÎ£å
            - Ïù∏Ïãù Ïã§Ìå® Ïãú ÏóêÎü¨ Î°úÍ∑∏ Í∏∞Î°ù ÌõÑ Í≥ÑÏÜç ÏßÑÌñâ
            - 25KB Ï≤≠ÌÅ¨ Ï†úÌïú Ï§ÄÏàò
            - v2 APIÎäî recognizer ÌååÎùºÎØ∏ÌÑ∞ ÌïÑÏàò
            - ÎèôÍ∏∞ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î•º Ïä§Î†àÎìúÏóêÏÑú Ïã§ÌñâÌïòÏó¨ ÎπÑÎèôÍ∏∞ Ìò∏Ìôò

        Examples:
            >>> audio_queue = asyncio.Queue()
            >>> service = STTService()
            >>> async for text in service.process_audio_stream(audio_queue):
            ...     await websocket.send_json({
            ...         "type": "transcript",
            ...         "data": {"text": text}
            ...     })
        """
        streaming_config = self._create_streaming_config()

        # Thread-safe queue to bridge asyncio and sync code
        sync_queue = queue.Queue()
        stop_event = threading.Event()

        # Background task to transfer frames from asyncio queue to sync queue
        async def transfer_frames():
            """asyncio QueueÏóêÏÑú thread-safe QueueÎ°ú ÌîÑÎ†àÏûÑ Ï†ÑÏÜ°"""
            chunk_count = 0
            try:
                logger.info("üéß Starting frame transfer task...")
                while not stop_event.is_set():
                    frame = await audio_queue.get()
                    if frame is None:
                        logger.info("Audio stream ended (received None)")
                        sync_queue.put(None)
                        break

                    chunk_count += 1
                    if chunk_count == 1:
                        logger.info(f"‚úÖ First audio frame received! Starting transfer...")
                    if chunk_count % 50 == 0:
                        logger.info(f"üì¶ Processing audio chunk #{chunk_count}")

                    sync_queue.put(frame)
                logger.info(f"Frame transfer completed. Total chunks: {chunk_count}")
            except Exception as e:
                logger.error(f"Error in frame transfer: {e}", exc_info=True)
                sync_queue.put(None)

        # Start frame transfer task
        transfer_task = asyncio.create_task(transfer_frames())

        def generate_requests():
            """ÎèôÍ∏∞ ÏöîÏ≤≠ ÏÉùÏÑ±Í∏∞ (v2 Î∞©Ïãù)"""
            # First request with recognizer and config
            logger.info("üì§ Sending initial config request to STT API...")
            config_request = cloud_speech.StreamingRecognizeRequest(
                recognizer=self.recognizer,
                streaming_config=streaming_config,
            )
            yield config_request
            logger.info("‚úÖ Config request sent, waiting for audio frames...")

            # Subsequent requests with audio data
            frame_count = 0
            last_frame_time = None
            silence_threshold = 2.0  # seconds of silence before closing
            first_frame_timeout = 10.0  # Wait longer for first frame

            while True:
                try:
                    # Wait longer for first frame, shorter for subsequent frames
                    timeout = first_frame_timeout if frame_count == 0 else 0.5
                    frame = sync_queue.get(timeout=timeout)
                except queue.Empty:
                    # Check if we should close stream due to prolonged silence
                    if last_frame_time is not None:
                        import time
                        silence_duration = time.time() - last_frame_time
                        if silence_duration > silence_threshold:
                            logger.info(f"‚è±Ô∏è No audio for {silence_duration:.1f}s, closing stream gracefully...")
                            break
                    elif frame_count == 0:
                        # No frames received at all after long wait
                        logger.error(f"‚ùå No audio frames received after {first_frame_timeout}s timeout!")
                        break
                    continue

                if frame is None:
                    logger.info(f"üèÅ Stream end signal received. Total frames sent: {frame_count}")
                    break

                # Update last frame time
                import time
                last_frame_time = time.time()

                frame_count += 1
                if frame_count == 1:
                    logger.info("üì§ Sending first audio frame to STT API...")

                # Convert frame to bytes (sync version)
                array = frame.to_ndarray()

                # Debug: Log frame info on first frame
                if frame_count == 1:
                    logger.info(f"üîç AudioFrame info - sample_rate: {frame.sample_rate}, format: {frame.format.name}, samples: {frame.samples}, channels: {frame.layout.name}")
                    logger.info(f"üîç Original array - shape: {array.shape}, dtype: {array.dtype}")

                # üîß FIX: Handle stereo to mono conversion properly
                # First flatten if multi-dimensional
                if array.ndim > 1:
                    array = array.flatten()

                # Check if size suggests stereo (should be 2x samples for interleaved L-R-L-R)
                if array.size == frame.samples * 2:
                    # Interleaved stereo: reshape to (samples, 2) and average channels
                    array = array.reshape(-1, 2).mean(axis=1).astype(array.dtype)
                    if frame_count == 1:
                        logger.info(f"üîß Converted stereo (interleaved) to mono: {frame.samples * 2} ‚Üí {frame.samples} samples")

                if frame_count == 1:
                    logger.info(f"üîç After conversion - shape: {array.shape}, dtype: {array.dtype}, min: {array.min()}, max: {array.max()}")

                # üîß Handle audio format conversion
                if array.dtype == np.float32 or array.dtype == np.float64:
                    # Float format (-1.0 to 1.0) - convert to int16
                    array = (array * 32767).astype(np.int16)
                    if frame_count == 1:
                        logger.info(f"üîß Converted float to int16 - min: {array.min()}, max: {array.max()}")
                elif array.dtype == np.int16:
                    # üîß CRITICAL FIX: Apply gain to low volume audio
                    max_val = np.abs(array).max()
                    if max_val > 0 and max_val < 5000:
                        # Audio is too quiet - apply gain
                        # Target: 20% of full range (~6500) for good recognition
                        gain = min(6500.0 / max_val, 20.0)  # Cap gain at 20x to avoid noise amplification
                        array = np.clip(array * gain, -32768, 32767).astype(np.int16)
                        if frame_count == 1:
                            logger.info(f"üîä Applied gain {gain:.1f}x - new range: [{array.min()}, {array.max()}]")

                audio_bytes = array.tobytes()

                # Debug: Log first frame audio data
                if frame_count == 1:
                    chunk_size = len(audio_bytes)
                    non_zero = np.count_nonzero(array)
                    logger.info(f"üîç Final audio - bytes: {chunk_size}, non-zero: {non_zero}/{array.size} ({100*non_zero/array.size:.1f}%), range: [{array.min()}, {array.max()}]")

                chunk_size = len(audio_bytes)
                if chunk_size > 25000:
                    logger.warning(f"Audio chunk size {chunk_size} exceeds 25KB limit, splitting...")
                    for i in range(0, len(audio_bytes), 24000):
                        chunk = audio_bytes[i:i+24000]
                        yield cloud_speech.StreamingRecognizeRequest(audio=chunk)
                else:
                    if frame_count % 100 == 0:
                        logger.debug(f"Sent frame #{frame_count} ({chunk_size} bytes)")
                    yield cloud_speech.StreamingRecognizeRequest(audio=audio_bytes)

        # Result queue to get transcripts from thread
        result_queue = queue.Queue()

        def run_streaming_recognize():
            """ÎèôÍ∏∞ STT Ìò∏Ï∂úÏùÑ Ïä§Î†àÎìúÏóêÏÑú Ïã§Ìñâ"""
            try:
                logger.info(f"üéôÔ∏è Starting streaming recognition with recognizer: {self.recognizer}")

                responses_iterator = self.client.streaming_recognize(
                    requests=generate_requests()
                )

                logger.info("‚úÖ STT stream connection established, waiting for responses...")
                logger.info("‚è≥ Waiting for STT API responses (this may take a few seconds)...")
                logger.info("üí° TIP: Speak clearly and pause after each phrase to get results")

                response_count = 0
                wait_logged = False
                for response in responses_iterator:
                    if not wait_logged and response_count == 0:
                        logger.info("üéØ Entering response loop, waiting for first response...")
                        wait_logged = True
                    response_count += 1
                    logger.info(f"üì® Received response #{response_count} from STT API")
                    logger.debug(f"Response type: {type(response)}, has results: {bool(response.results)}")

                    if not response.results:
                        logger.debug(f"Response #{response_count} has no results, skipping...")
                        continue

                    result = response.results[0]

                    if result.is_final or self.enable_interim_results:
                        if result.alternatives:
                            transcript = result.alternatives[0].transcript
                            confidence = result.alternatives[0].confidence if result.is_final else 0.0

                            result_type = "FINAL" if result.is_final else "INTERIM"
                            logger.info(
                                f"STT Result ({result_type}): '{transcript}' "
                                f"(confidence: {confidence:.2f})"
                            )

                            # Only send final results to frontend (ignore interim)
                            if result.is_final:
                                result_queue.put(transcript)

                # Signal end of stream
                result_queue.put(None)

            except Exception as e:
                # Google STT APIÏùò Ïä§Ìä∏Î¶º Ï†úÌïú ÎèÑÎã¨ Ïãú 500 ÏóêÎü¨ Î∞úÏÉù (Ï†ïÏÉÅÏ†ÅÏù∏ Ï¢ÖÎ£å)
                if "500" in str(e) or "Internal error" in str(e):
                    logger.info(f"üîÑ STT stream limit reached (normal behavior), will restart: {e}")
                else:
                    logger.error(f"‚ùå Unexpected STT error: {e}", exc_info=True)
                result_queue.put(None)

        try:
            # Start STT processing in background thread
            stt_thread = threading.Thread(target=run_streaming_recognize, daemon=True)
            stt_thread.start()

            # Yield results as they arrive
            while True:
                # Get result from queue (with timeout to check stop event)
                try:
                    transcript = await asyncio.to_thread(result_queue.get, timeout=0.1)
                    if transcript is None:
                        break
                    yield transcript
                except queue.Empty:
                    if stop_event.is_set():
                        break
                    continue

        except Exception as e:
            logger.error(f"Error in process_audio_stream: {e}", exc_info=True)
            raise
        finally:
            stop_event.set()
            await transfer_task
            # Wait for STT thread to finish
            await asyncio.to_thread(stt_thread.join, timeout=5)

    async def recognize_single_audio(self, audio_bytes: bytes) -> Optional[str]:
        """Îã®Ïùº Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞Î•º Ïù∏Ïãù (ÎπÑÏä§Ìä∏Î¶¨Î∞ç).

        ÏßßÏùÄ Ïò§ÎîîÏò§ ÌÅ¥Î¶ΩÏùÑ Ìïú Î≤àÏóê Ïù∏ÏãùÌï©ÎãàÎã§.
        v2 APIÏóêÏÑúÎäî recognize() Î©îÏÑúÎìúÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.

        Args:
            audio_bytes (bytes): 16-bit PCM Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞

        Returns:
            Optional[str]: Ïù∏ÏãùÎêú ÌÖçÏä§Ìä∏. Ïù∏Ïãù Ïã§Ìå® Ïãú None

        Note:
            - ÏµúÎåÄ 60Ï¥à Ïò§ÎîîÏò§ Í∂åÏû•
            - Ïã§ÏãúÍ∞Ñ Ïö©ÎèÑÎ°úÎäî process_audio_stream() ÏÇ¨Ïö© Í∂åÏû•
            - v2ÏóêÏÑúÎäî batch recognition ÏÇ¨Ïö© Í∞ÄÎä•

        Examples:
            >>> service = STTService()
            >>> with open("audio.pcm", "rb") as f:
            ...     audio = f.read()
            >>> text = await service.recognize_single_audio(audio)
            >>> print(text)
        """
        try:
            # RecognitionConfig ÏÉùÏÑ±
            config = cloud_speech.RecognitionConfig(
                auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),
                language_codes=self.language_codes,
                model=self.model,
            )

            if self.enable_automatic_punctuation:
                config.features = cloud_speech.RecognitionFeatures(
                    enable_automatic_punctuation=True
                )

            # RecognizeRequest ÏÉùÏÑ± (v2 Î∞©Ïãù)
            request = cloud_speech.RecognizeRequest(
                recognizer=self.recognizer,
                config=config,
                content=audio_bytes,
            )

            # Synchronous recognition in thread
            response = await asyncio.to_thread(self.client.recognize, request=request)

            if response.results:
                transcript = response.results[0].alternatives[0].transcript
                logger.info(f"Single audio STT v2 result: '{transcript}'")
                return transcript

            return None

        except Exception as e:
            logger.error(f"Error in single audio recognition (v2): {e}")
            return None
