"""Google Cloud Speech-to-Text v2 ÏÑúÎπÑÏä§ ÌÜµÌï© Î™®Îìà.

Ïù¥ Î™®ÎìàÏùÄ Google Cloud Speech-to-Text API v2Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ïã§ÏãúÍ∞Ñ Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶ºÏùÑ
ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôòÌïòÎäî Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.

Ï£ºÏöî Í∏∞Îä•:
    - Ïã§ÏãúÍ∞Ñ Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º Ïù∏Ïãù (Streaming Recognition)
    - WebRTC Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑÏùÑ Google STT API ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò
    - ÎπÑÎèôÍ∏∞ Ï≤òÎ¶¨Î•º ÌÜµÌïú ÎÜíÏùÄ Ï≤òÎ¶¨Îüâ
    - ÌïúÍµ≠Ïñ¥ ÏùåÏÑ± Ïù∏Ïãù ÏµúÏ†ÅÌôî

Architecture:
    - Google Cloud Speech-to-Text API v2 ÏÇ¨Ïö©
    - Recognizer Í∏∞Î∞ò Ïä§Ìä∏Î¶¨Î∞ç Ïù∏Ïãù
    - AudioFrame ‚Üí PCM bytes Î≥ÄÌôò ÌååÏù¥ÌîÑÎùºÏù∏
    - ÏûêÎèô Íµ¨ÎëêÏ†ê Î∞è Ïã§ÏãúÍ∞Ñ Í≤∞Í≥º ÏßÄÏõê

Examples:
    Í∏∞Î≥∏ ÏÇ¨Ïö©Î≤ï:
        >>> service = STTService()
        >>> async for text in service.process_audio_stream(audio_frames):
        ...     print(f"Ïù∏ÏãùÎêú ÌÖçÏä§Ìä∏: {text}")

    Ïª§Ïä§ÌÖÄ ÏÑ§Ï†ï:
        >>> service = STTService(
        ...     language_codes=["ko-KR"],
        ...     model="chirp",
        ...     enable_automatic_punctuation=True
        ... )

See Also:
    peer_manager.py: Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑ Ï∫°Ï≤ò
    app.py: WebSocketÏùÑ ÌÜµÌïú Í≤∞Í≥º Ï†ÑÏÜ°
    Google Cloud Speech-to-Text V2 Documentation:
        https://cloud.google.com/speech-to-text/v2/docs
"""
import asyncio
import logging
import os
from typing import AsyncIterator, Optional, List
from google.cloud.speech_v2 import SpeechClient
from google.cloud.speech_v2.types import cloud_speech
from google.api_core.client_options import ClientOptions
from google.protobuf.duration_pb2 import Duration
from av import AudioFrame
import numpy as np
import queue
import threading

logger = logging.getLogger(__name__)

# Google STT ÏµúÏ†ÅÌôî ÏÉÅÏàò
TARGET_SAMPLE_RATE = 16000  # ElevenLabsÏôÄ ÎèôÏùºÌïòÍ≤å 16kHz
TARGET_CHUNK_DURATION = 0.25  # 250ms Ï≤≠ÌÅ¨ (ElevenLabsÏôÄ ÎèôÏùº)


class STTService:
    """Google Cloud Speech-to-Text v2 ÏÑúÎπÑÏä§ ÎûòÌçº ÌÅ¥ÎûòÏä§.

    WebRTC Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶ºÏùÑ Ïã§ÏãúÍ∞ÑÏúºÎ°ú ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.
    v2 APIÏùò Recognizer Í∏∞Î∞ò Ïä§Ìä∏Î¶¨Î∞ç Ïù∏ÏãùÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.

    Attributes:
        client (SpeechClient): Google Cloud Speech v2 ÎèôÍ∏∞ API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏
        project_id (str): Google Cloud ÌîÑÎ°úÏ†ùÌä∏ ID
        recognizer (str): Recognizer Î¶¨ÏÜåÏä§ Í≤ΩÎ°ú
        language_codes (List[str]): ÏùåÏÑ± Ïù∏Ïãù Ïñ∏Ïñ¥ ÏΩîÎìú Î¶¨Ïä§Ìä∏
        model (str): ÏÇ¨Ïö©Ìï† ÏùåÏÑ± Ïù∏Ïãù Î™®Îç∏
        enable_automatic_punctuation (bool): ÏûêÎèô Íµ¨ÎëêÏ†ê Ï∂îÍ∞Ä Ïó¨Î∂Ä
        enable_interim_results (bool): Ï§ëÍ∞Ñ Í≤∞Í≥º ÌôúÏÑ±Ìôî Ïó¨Î∂Ä

    Note:
        - GOOGLE_APPLICATION_CREDENTIALS ÌôòÍ≤Ω Î≥ÄÏàò ÌïÑÏàò
        - GOOGLE_CLOUD_PROJECT ÌôòÍ≤Ω Î≥ÄÏàò ÌïÑÏàò (ÌîÑÎ°úÏ†ùÌä∏ ID)
        - WebRTC Ïò§ÎîîÏò§Îäî ÏûêÎèôÏúºÎ°ú Ïù∏ÏΩîÎî© Í∞êÏßÄÎê®
        - 25KB Ïä§Ìä∏Î¶º Ï†úÌïú Ï£ºÏùò

    Examples:
        >>> service = STTService()
        >>> # Ïò§ÎîîÏò§ Ïä§Ìä∏Î¶º Ï≤òÎ¶¨
        >>> async for transcript in service.process_audio_stream(audio_queue):
        ...     print(f"Ïù∏Ïãù Í≤∞Í≥º: {transcript}")
    """

    def __init__(
        self,
        project_id: Optional[str] = None,
        language_codes: Optional[List[str]] = None,
        model: Optional[str] = None,
        enable_automatic_punctuation: Optional[bool] = None,
        enable_interim_results: Optional[bool] = None,
    ):
        """STTService Ï¥àÍ∏∞Ìôî.

        Args:
            project_id (str, optional): Google Cloud ÌîÑÎ°úÏ†ùÌä∏ ID.
                ÌôòÍ≤Ω Î≥ÄÏàò GOOGLE_CLOUD_PROJECT ÎòêÎäî ÌïÑÏàò
            language_codes (List[str], optional): ÏùåÏÑ± Ïù∏Ïãù Ïñ∏Ïñ¥ ÏΩîÎìú Î¶¨Ïä§Ìä∏.
                ÌôòÍ≤Ω Î≥ÄÏàò STT_LANGUAGE_CODE ÎòêÎäî ["ko-KR"] ÏÇ¨Ïö©
            model (str, optional): ÏùåÏÑ± Ïù∏Ïãù Î™®Îç∏.
                ÌôòÍ≤Ω Î≥ÄÏàò STT_MODEL ÎòêÎäî "chirp" ÏÇ¨Ïö©
            enable_automatic_punctuation (bool, optional): ÏûêÎèô Íµ¨ÎëêÏ†ê Ï∂îÍ∞Ä.
                ÌôòÍ≤Ω Î≥ÄÏàò STT_ENABLE_AUTOMATIC_PUNCTUATION ÎòêÎäî True ÏÇ¨Ïö©
            enable_interim_results (bool, optional): Ï§ëÍ∞Ñ Í≤∞Í≥º ÌôúÏÑ±Ìôî.
                ÌôòÍ≤Ω Î≥ÄÏàò STT_ENABLE_INTERIM_RESULTS ÎòêÎäî False ÏÇ¨Ïö©

        Raises:
            ValueError: GOOGLE_CLOUD_PROJECT ÎØ∏ÏÑ§Ï†ï Ïãú

        Note:
            - .env ÌååÏùºÏóêÏÑú ÌôòÍ≤Ω Î≥ÄÏàò Î°úÎìú ÌïÑÏöî
            - ÏÑúÎπÑÏä§ Í≥ÑÏ†ï ÌÇ§ ÌååÏùº Í∂åÌïú ÌôïÏù∏ ÌïÑÏöî
            - v2 APIÎäî Recognizer Í∞úÎÖê ÌïÑÏàò
        """
        # Google Cloud Ïù∏Ï¶ù Î∞è ÌîÑÎ°úÏ†ùÌä∏ ÌôïÏù∏ ÌîÑÎ°úÏ†ùÌä∏ ID (v2ÏóêÏÑú ÌïÑÏàò)
        if not os.getenv("GOOGLE_APPLICATION_CREDENTIALS"):
            logger.warning(
                "GOOGLE_APPLICATION_CREDENTIALS not set. "
                "STT service may not work properly."
            )
            
        self.project_id = project_id or os.getenv("GOOGLE_CLOUD_PROJECT")
        if not self.project_id:
            raise ValueError(
                "GOOGLE_CLOUD_PROJECT environment variable must be set for v2 API"
            )

        # Configuration from environment or defaults
        default_language = os.getenv("STT_LANGUAGE_CODE", "ko-KR")
        self.language_codes = language_codes or [default_language]

        self.model = model or os.getenv("STT_MODEL", "short")
        self.sample_rate = TARGET_SAMPLE_RATE
        self.input_sample_rate = int(os.getenv("STT_SAMPLE_RATE_HERTZ", "32000"))  # WebRTC ÏûÖÎ†• ÏÉòÌîåÎ†àÏù¥Ìä∏ (32kbps ÎπÑÌä∏Î†àÏù¥Ìä∏Ïóê ÏµúÏ†ÅÌôî)

        # Location ÏÑ§Ï†ï (Î¶¨Ï†ÑÎ≥Ñ ÏóîÎìúÌè¨Ïù∏Ìä∏ ÏßÄÏõê)
        self.location = os.getenv("STT_LOCATION", "global")

        # Regional endpoint ÏÑ§Ï†ï
        if self.location != "global":
            api_endpoint = f"{self.location}-speech.googleapis.com"
            client_options = ClientOptions(api_endpoint=api_endpoint)
            self.client = SpeechClient(client_options=client_options)
            logger.info(f"üåè Using regional endpoint: {api_endpoint}")
        else:
            self.client = SpeechClient()
            logger.info("üåê Using global endpoint: speech.googleapis.com")

        # Recognizer path
        self.recognizer = f"projects/{self.project_id}/locations/{self.location}/recognizers/_"

        self.enable_automatic_punctuation = (
            enable_automatic_punctuation
            if enable_automatic_punctuation is not None
            else os.getenv("STT_ENABLE_AUTOMATIC_PUNCTUATION", "true").lower() == "true"
        )

        # Enable interim results for real-time partial transcript display
        self.enable_interim_results = (
            enable_interim_results
            if enable_interim_results is not None
            else os.getenv("STT_ENABLE_INTERIM_RESULTS", "true").lower() == "true"
        )

        logger.info(
            f"STT Service v2 initialized: "
            f"project={self.project_id}, "
            f"location={self.location}, "
            f"languages={self.language_codes}, "
            f"model={self.model}, "
            f"sample_rate={self.sample_rate}Hz, "
            f"punctuation={self.enable_automatic_punctuation}, "
            f"interim={self.enable_interim_results}"
        )

    def _resample_audio(self, audio_array: np.ndarray, orig_sr: int, target_sr: int) -> np.ndarray:
        """Ïò§ÎîîÏò§Î•º Î™©Ìëú ÏÉòÌîåÎ†àÏù¥Ìä∏Î°ú Î¶¨ÏÉòÌîåÎßÅÌï©ÎãàÎã§.

        Í∞ÑÎã®Ìïú ÏÑ†Ìòï Î≥¥Í∞Ñ Î∞©ÏãùÏùÑ ÏÇ¨Ïö©Ìï©ÎãàÎã§.

        Args:
            audio_array: ÏõêÎ≥∏ Ïò§ÎîîÏò§ Î∞∞Ïó¥
            orig_sr: ÏõêÎ≥∏ ÏÉòÌîåÎ†àÏù¥Ìä∏
            target_sr: Î™©Ìëú ÏÉòÌîåÎ†àÏù¥Ìä∏

        Returns:
            Î¶¨ÏÉòÌîåÎßÅÎêú Ïò§ÎîîÏò§ Î∞∞Ïó¥
        """
        if orig_sr == target_sr:
            return audio_array

        # Î¶¨ÏÉòÌîåÎßÅ ÎπÑÏú® Í≥ÑÏÇ∞
        ratio = target_sr / orig_sr
        new_length = int(len(audio_array) * ratio)

        # ÏÑ†Ìòï Î≥¥Í∞ÑÏúºÎ°ú Î¶¨ÏÉòÌîåÎßÅ
        indices = np.linspace(0, len(audio_array) - 1, new_length)
        resampled = np.interp(indices, np.arange(len(audio_array)), audio_array)

        return resampled.astype(audio_array.dtype)

    def _create_streaming_config(self) -> cloud_speech.StreamingRecognitionConfig:
        """Ïä§Ìä∏Î¶¨Î∞ç Ïù∏ÏãùÏùÑ ÏúÑÌïú Google STT v2 ÏÑ§Ï†ï ÏÉùÏÑ±.

        Returns:
            cloud_speech.StreamingRecognitionConfig: Ïä§Ìä∏Î¶¨Î∞ç Ïù∏Ïãù ÏÑ§Ï†ï Í∞ùÏ≤¥

        Note:
            - ExplicitDecodingConfig: WebRTC Ïò§ÎîîÏò§ ÌòïÏãù Î™ÖÏãúÏ†Å ÏßÄÏ†ï
            - language_codes: Îã§Ï§ë Ïñ∏Ïñ¥ ÏßÄÏõê (Î¶¨Ïä§Ìä∏)
            - model: latest_long Îì±
            - interim_results: Ïã§ÏãúÍ∞Ñ Ï§ëÍ∞Ñ Í≤∞Í≥º (False Í∂åÏû• - ÎÇÆÏùÄ ÏßÄÏó∞ÏãúÍ∞Ñ)
        """
        recognition_config = cloud_speech.RecognitionConfig(
            explicit_decoding_config=cloud_speech.ExplicitDecodingConfig(
                encoding=cloud_speech.ExplicitDecodingConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=self.sample_rate,
                audio_channel_count=1,
            ),
            language_codes=self.language_codes,
            model=self.model,
        )

        # Features ÏÑ§Ï†ï (Íµ¨ÎëêÏ†ê Îì±)
        if self.enable_automatic_punctuation:
            recognition_config.features = cloud_speech.RecognitionFeatures(
                enable_automatic_punctuation=True
            )

        # StreamingRecognitionConfig ÏÉùÏÑ±
        streaming_config = cloud_speech.StreamingRecognitionConfig(
            config=recognition_config,
        )

        # StreamingRecognitionFeatures Ï∂îÍ∞Ä (interim results + voice activity timeout)
        streaming_config.streaming_features = cloud_speech.StreamingRecognitionFeatures(
            interim_results=self.enable_interim_results,
            enable_voice_activity_events=True,
            voice_activity_timeout=cloud_speech.StreamingRecognitionFeatures.VoiceActivityTimeout(
                speech_end_timeout=Duration(seconds=59),  # Î∞úÌôî ÌõÑ 59Ï¥àÍπåÏßÄ Ïä§Ìä∏Î¶º Ïú†ÏßÄ
            ),
        )

        return streaming_config

    async def _audio_frame_to_bytes(self, frame: AudioFrame) -> bytes:
        """AudioFrameÏùÑ Google STT API ÌòïÏãùÏùò PCM bytesÎ°ú Î≥ÄÌôò.

        WebRTC AudioFrameÏùÑ 16-bit PCM Î∞îÏù¥Ìä∏ Î∞∞Ïó¥Î°ú Î≥ÄÌôòÌï©ÎãàÎã§.

        Args:
            frame (AudioFrame): WebRTC Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑ

        Returns:
            bytes: 16-bit PCM Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞

        Note:
            - AudioFrame.to_ndarray()Î°ú numpy Î∞∞Ïó¥ Ï∂îÏ∂ú
            - int16 ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò (Google STT ÏöîÍµ¨ÏÇ¨Ìï≠)
            - Ïä§ÌÖåÎ†àÏò§Îäî Î™®ÎÖ∏Î°ú Î≥ÄÌôò (Ï±ÑÎÑê ÌèâÍ∑†)
            - ÎÇÆÏùÄ Î≥ºÎ•® ÏûêÎèô Ï¶ùÌè≠
        """
        # Convert AudioFrame to numpy array
        array = frame.to_ndarray()

        # Handle stereo to mono conversion properly
        # First flatten if multi-dimensional
        if array.ndim > 1:
            array = array.flatten()

        # Check if size suggests stereo (2x samples for interleaved L-R-L-R)
        if array.size == frame.samples * 2:
            # Interleaved stereo: reshape to (samples, 2) and average channels
            array = array.reshape(-1, 2).mean(axis=1).astype(array.dtype)

        # Handle WebRTC audio format conversion
        if array.dtype in (np.float32, np.float64):
            # Float format - convert to int16
            array = (array * 32767).astype(np.int16)
        elif array.dtype == np.int16:
            # Apply gain to low volume audio
            max_val = np.abs(array).max()
            if max_val > 0 and max_val < 8000:
                # Target: 30% of full range (~10000) for reliable STT recognition
                target_level = 10000.0
                gain = min(target_level / max_val, 150.0)  # Cap gain at 150x for very quiet audio
                array = np.clip(array * gain, -32768, 32767).astype(np.int16)

        return array.tobytes()

    async def process_audio_stream(
        self,
        audio_queue: asyncio.Queue
    ) -> AsyncIterator[dict]:
        """Ïò§ÎîîÏò§ ÌîÑÎ†àÏûÑ ÌÅêÎ•º Ï≤òÎ¶¨ÌïòÏó¨ ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôò.

        ÎπÑÎèôÍ∏∞ Ï†úÎÑàÎ†àÏù¥ÌÑ∞Î°ú Ïó∞ÏÜçÏ†ÅÏù∏ ÏùåÏÑ± Ïù∏Ïãù Í≤∞Í≥ºÎ•º Ïä§Ìä∏Î¶¨Î∞çÌï©ÎãàÎã§.

        Args:
            audio_queue (asyncio.Queue): AudioFrame Í∞ùÏ≤¥Î•º Îã¥ÏùÄ ÎπÑÎèôÍ∏∞ ÌÅê

        Yields:
            dict: Ïù∏Ïãù Í≤∞Í≥º
                - transcript (str): Ïù∏ÏãùÎêú ÌÖçÏä§Ìä∏
                - is_final (bool): ÏµúÏ¢Ö Í≤∞Í≥º Ïó¨Î∂Ä
                - confidence (float): Ïã†Î¢∞ÎèÑ Ï†êÏàò

        Note:
            - ÌÅêÏóêÏÑú NoneÏùÑ Î∞õÏúºÎ©¥ Ïä§Ìä∏Î¶º Ï¢ÖÎ£å
            - Ïù∏Ïãù Ïã§Ìå® Ïãú ÏóêÎü¨ Î°úÍ∑∏ Í∏∞Î°ù ÌõÑ Í≥ÑÏÜç ÏßÑÌñâ
            - 25KB Ï≤≠ÌÅ¨ Ï†úÌïú Ï§ÄÏàò
            - v2 APIÎäî recognizer ÌååÎùºÎØ∏ÌÑ∞ ÌïÑÏàò
            - ÎèôÍ∏∞ ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏Î•º Ïä§Î†àÎìúÏóêÏÑú Ïã§ÌñâÌïòÏó¨ ÎπÑÎèôÍ∏∞ Ìò∏Ìôò

        Examples:
            >>> audio_queue = asyncio.Queue()
            >>> service = STTService()
            >>> async for text in service.process_audio_stream(audio_queue):
            ...     await websocket.send_json({
            ...         "type": "transcript",
            ...         "data": {"text": text}
            ...     })
        """
        streaming_config = self._create_streaming_config()

        # Thread-safe queue to bridge asyncio and sync code
        sync_queue = queue.Queue()
        stop_event = threading.Event()

        # Background task to transfer frames from asyncio queue to sync queue
        async def transfer_frames():
            """asyncio QueueÏóêÏÑú thread-safe QueueÎ°ú ÌîÑÎ†àÏûÑ Ï†ÑÏÜ°"""
            chunk_count = 0
            try:
                logger.info("üéß Starting frame transfer task...")
                while not stop_event.is_set():
                    frame = await audio_queue.get()
                    if frame is None:
                        logger.info("Audio stream ended (received None)")
                        sync_queue.put(None)
                        break

                    chunk_count += 1
                    if chunk_count == 1:
                        logger.info(f"‚úÖ First audio frame received! Starting transfer...")
                    if chunk_count % 50 == 0:
                        logger.info(f"üì¶ Processing audio chunk #{chunk_count}")

                    sync_queue.put(frame)
                logger.info(f"Frame transfer completed. Total chunks: {chunk_count}")
            except Exception as e:
                logger.error(f"Error in frame transfer: {e}", exc_info=True)
                sync_queue.put(None)

        # Start frame transfer task
        transfer_task = asyncio.create_task(transfer_frames())

        def generate_requests():
            """ÎèôÍ∏∞ ÏöîÏ≤≠ ÏÉùÏÑ±Í∏∞ (v2 Î∞©Ïãù) - 250ms Ï≤≠ÌÅ¨ ÎàÑÏ†Å Ï†ÑÏÜ°"""
            # First request with recognizer and config
            logger.info("üì§ Sending initial config request to STT API...")
            config_request = cloud_speech.StreamingRecognizeRequest(
                recognizer=self.recognizer,
                streaming_config=streaming_config,
            )
            yield config_request
            logger.info("‚úÖ Config request sent, waiting for audio frames...")

            # üîß ElevenLabsÏôÄ ÎèôÏùºÌïú Ï≤≠ÌÅ¨ ÎàÑÏ†Å Î∞©Ïãù
            frame_count = 0
            chunk_count = 0
            accumulated_arrays = []
            accumulated_duration = 0.0
            last_frame_time = None
            silence_threshold = 30.0
            first_frame_timeout = 60.0

            def process_accumulated_chunks():
                """ÎàÑÏ†ÅÎêú Ïò§ÎîîÏò§Î•º Ï≤òÎ¶¨ÌïòÏó¨ Ï†ÑÏÜ°"""
                nonlocal chunk_count
                if not accumulated_arrays:
                    return None

                # Î™®Îì† Î∞∞Ïó¥ÏùÑ ÌïòÎÇòÎ°ú Ìï©Ïπ®
                combined_array = np.concatenate(accumulated_arrays)

                # 48kHz ‚Üí 16kHz Î¶¨ÏÉòÌîåÎßÅ (ElevenLabsÏôÄ ÎèôÏùº)
                resampled = self._resample_audio(combined_array, self.input_sample_rate, self.sample_rate)
                resampled = resampled.astype(np.int16)

                chunk_count += 1
                if chunk_count == 1:
                    logger.info(f"üì§ First 250ms chunk: {len(combined_array)} samples @ 48kHz ‚Üí {len(resampled)} samples @ 16kHz")

                audio_bytes = resampled.tobytes()
                return audio_bytes

            while True:
                try:
                    # Wait longer for first frame, shorter for subsequent frames
                    timeout = first_frame_timeout if frame_count == 0 else 0.1
                    frame = sync_queue.get(timeout=timeout)
                except queue.Empty:
                    # ÌÉÄÏûÑÏïÑÏõÉ Ïãú ÎàÑÏ†ÅÎêú Ïò§ÎîîÏò§Í∞Ä ÏûàÏúºÎ©¥ Ï†ÑÏÜ°
                    if accumulated_arrays:
                        audio_bytes = process_accumulated_chunks()
                        if audio_bytes:
                            yield cloud_speech.StreamingRecognizeRequest(audio=audio_bytes)
                        accumulated_arrays = []
                        accumulated_duration = 0.0

                    # Check if we should close stream due to prolonged silence
                    if last_frame_time is not None:
                        import time
                        silence_duration = time.time() - last_frame_time
                        if silence_duration > silence_threshold:
                            logger.info(f"‚è±Ô∏è No audio for {silence_duration:.1f}s, closing stream gracefully...")
                            break
                    elif frame_count == 0:
                        # No frames received at all after long wait
                        logger.error(f"‚ùå No audio frames received after {first_frame_timeout}s timeout!")
                        break
                    continue

                if frame is None:
                    # Ïä§Ìä∏Î¶º Ï¢ÖÎ£å Ï†Ñ ÎÇ®ÏùÄ Ïò§ÎîîÏò§ Ï†ÑÏÜ°
                    if accumulated_arrays:
                        audio_bytes = process_accumulated_chunks()
                        if audio_bytes:
                            yield cloud_speech.StreamingRecognizeRequest(audio=audio_bytes)
                    logger.info(f"üèÅ Stream end signal received. Total frames: {frame_count}, chunks sent: {chunk_count}")
                    break

                # Update last frame time
                import time
                last_frame_time = time.time()

                frame_count += 1
                if frame_count == 1:
                    logger.info(f"üîç AudioFrame info - sample_rate: {frame.sample_rate}, format: {frame.format.name}, samples: {frame.samples}")

                # Convert frame to numpy array
                array = frame.to_ndarray()

                # üîß Handle stereo to mono conversion
                if array.ndim > 1:
                    array = array.flatten()

                if array.size == frame.samples * 2:
                    array = array.reshape(-1, 2).mean(axis=1).astype(array.dtype)
                    if frame_count == 1:
                        logger.info(f"üîß Converted stereo to mono")

                # üîß Handle audio format conversion
                if array.dtype == np.float32 or array.dtype == np.float64:
                    array = (array * 32767).astype(np.int16)
                elif array.dtype == np.int16:
                    # Apply gain to low volume audio
                    max_val = np.abs(array).max()
                    if max_val > 0 and max_val < 5000:
                        gain = min(6500.0 / max_val, 20.0)
                        array = np.clip(array * gain, -32768, 32767).astype(np.int16)

                # ÌîÑÎ†àÏûÑ ÎàÑÏ†Å
                accumulated_arrays.append(array)
                frame_duration = frame.samples / frame.sample_rate
                accumulated_duration += frame_duration

                # 250ms Ïù¥ÏÉÅ ÎàÑÏ†ÅÎêòÎ©¥ Ï†ÑÏÜ° (ElevenLabsÏôÄ ÎèôÏùº)
                if accumulated_duration >= TARGET_CHUNK_DURATION:
                    audio_bytes = process_accumulated_chunks()
                    if audio_bytes:
                        chunk_size = len(audio_bytes)
                        if chunk_size > 25000:
                            logger.warning(f"Audio chunk size {chunk_size} exceeds 25KB limit, splitting...")
                            for i in range(0, len(audio_bytes), self.input_sample_rate):
                                chunk = audio_bytes[i:i+self.input_sample_rate]
                                yield cloud_speech.StreamingRecognizeRequest(audio=chunk)
                        else:
                            yield cloud_speech.StreamingRecognizeRequest(audio=audio_bytes)

                        if chunk_count % 40 == 0:  # ~10Ï¥àÎßàÎã§ Î°úÍ∑∏
                            logger.info(f"üì¶ Sent {chunk_count} chunks to Google STT")

                    # ÎàÑÏ†Å Ï¥àÍ∏∞Ìôî
                    accumulated_arrays = []
                    accumulated_duration = 0.0

        # Result queue to get transcripts from thread
        result_queue = queue.Queue()

        def run_streaming_recognize():
            """ÎèôÍ∏∞ STT Ìò∏Ï∂úÏùÑ Ïä§Î†àÎìúÏóêÏÑú Ïã§Ìñâ"""
            try:
                logger.info(f"üéôÔ∏è Starting streaming recognition with recognizer: {self.recognizer}")
                logger.info(f"üîó API endpoint: {self.client._transport._host if hasattr(self.client, '_transport') else 'unknown'}")

                logger.info("üì° Calling streaming_recognize()...")
                responses_iterator = self.client.streaming_recognize(
                    requests=generate_requests()
                )
                logger.info("üì° streaming_recognize() returned iterator, starting to iterate...")

                logger.info("‚è≥ Waiting for first response from STT API...")

                response_count = 0
                for response in responses_iterator:
                    if response_count == 0:
                        logger.info("‚úÖ STT stream connection established, first response received!")
                    else:
                        logger.info(f"‚è≥ Received response after waiting...")
                    response_count += 1
                    logger.info(f"üì® Received response #{response_count} from STT API")

                    if not response.results:
                        logger.info(f"üì≠ Response #{response_count} has no results (empty)")
                        continue

                    result = response.results[0]
                    logger.info(f"üì¨ Response #{response_count}: is_final={result.is_final}, alternatives={len(result.alternatives) if result.alternatives else 0}")

                    if result.is_final or self.enable_interim_results:
                        if result.alternatives:
                            transcript = result.alternatives[0].transcript
                            confidence = result.alternatives[0].confidence if result.is_final else 0.0

                            result_type = "FINAL" if result.is_final else "INTERIM"
                            logger.info(
                                f"STT Result ({result_type}): '{transcript}' "
                                f"(confidence: {confidence:.2f})"
                            )

                            # Send both final and interim results with is_final flag
                            result_queue.put({
                                "transcript": transcript,
                                "is_final": result.is_final,
                                "confidence": confidence
                            })

                    logger.info(f"‚è≥ Waiting for response #{response_count + 1}...")

                # Signal end of stream
                logger.info(f"üèÅ Response iterator ended. Total responses: {response_count}")
                result_queue.put(None)

            except Exception as e:
                # Google STT APIÏùò Ïä§Ìä∏Î¶º ÏûêÎèô Ï¢ÖÎ£å (Ï†ïÏÉÅÏ†ÅÏù∏ ÎèôÏûë)
                # 499 CANCELLED: GoogleÏù¥ Ïä§Ìä∏Î¶ºÏùÑ ÏûêÎèôÏúºÎ°ú Îã´Ïùå
                # 500 Internal error: Ïä§Ìä∏Î¶º Ï†úÌïú ÏãúÍ∞Ñ ÎèÑÎã¨
                if ("499" in str(e) or "CANCELLED" in str(e).upper() or
                    "500" in str(e) or "Internal error" in str(e)):
                    logger.info(f"üîÑ STT stream ended (normal behavior), will restart: {e}")
                else:
                    logger.error(f"‚ùå Unexpected STT error: {e}", exc_info=True)
                result_queue.put(None)

        try:
            # Start STT processing in background thread
            stt_thread = threading.Thread(target=run_streaming_recognize, daemon=True)
            stt_thread.start()

            # Yield results as they arrive
            while True:
                # Get result from queue (with timeout to check stop event)
                try:
                    result = await asyncio.to_thread(result_queue.get, timeout=0.1)
                    if result is None:
                        break
                    yield result  # dict with transcript, is_final, confidence
                except queue.Empty:
                    if stop_event.is_set():
                        break
                    continue

        except Exception as e:
            logger.error(f"Error in process_audio_stream: {e}", exc_info=True)
            raise
        finally:
            stop_event.set()
            await transfer_task
            # Wait for STT thread to finish
            await asyncio.to_thread(stt_thread.join, timeout=5)

    async def recognize_single_audio(self, audio_bytes: bytes) -> Optional[str]:
        """Îã®Ïùº Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞Î•º Ïù∏Ïãù (ÎπÑÏä§Ìä∏Î¶¨Î∞ç).

        ÏßßÏùÄ Ïò§ÎîîÏò§ ÌÅ¥Î¶ΩÏùÑ Ìïú Î≤àÏóê Ïù∏ÏãùÌï©ÎãàÎã§.
        v2 APIÏóêÏÑúÎäî recognize() Î©îÏÑúÎìúÎ•º ÏÇ¨Ïö©Ìï©ÎãàÎã§.

        Args:
            audio_bytes (bytes): 16-bit PCM Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞

        Returns:
            Optional[str]: Ïù∏ÏãùÎêú ÌÖçÏä§Ìä∏. Ïù∏Ïãù Ïã§Ìå® Ïãú None

        Note:
            - ÏµúÎåÄ 60Ï¥à Ïò§ÎîîÏò§ Í∂åÏû•
            - Ïã§ÏãúÍ∞Ñ Ïö©ÎèÑÎ°úÎäî process_audio_stream() ÏÇ¨Ïö© Í∂åÏû•
            - v2ÏóêÏÑúÎäî batch recognition ÏÇ¨Ïö© Í∞ÄÎä•

        Examples:
            >>> service = STTService()
            >>> with open("audio.pcm", "rb") as f:
            ...     audio = f.read()
            >>> text = await service.recognize_single_audio(audio)
            >>> print(text)
        """
        try:
            # RecognitionConfig ÏÉùÏÑ±
            config = cloud_speech.RecognitionConfig(
                auto_decoding_config=cloud_speech.AutoDetectDecodingConfig(),
                language_codes=self.language_codes,
                model=self.model,
            )

            if self.enable_automatic_punctuation:
                config.features = cloud_speech.RecognitionFeatures(
                    enable_automatic_punctuation=True
                )

            # RecognizeRequest ÏÉùÏÑ± (v2 Î∞©Ïãù)
            request = cloud_speech.RecognizeRequest(
                recognizer=self.recognizer,
                config=config,
                content=audio_bytes,
            )

            # Synchronous recognition in thread
            response = await asyncio.to_thread(self.client.recognize, request=request)

            if response.results:
                transcript = response.results[0].alternatives[0].transcript
                logger.info(f"Single audio STT v2 result: '{transcript}'")
                return transcript

            return None

        except Exception as e:
            logger.error(f"Error in single audio recognition (v2): {e}")
            return None
