"""LangGraph Agent for Real-time Conversation Summarization.

ì´ ëª¨ë“ˆì€ ì‹¤ì‹œê°„ ìƒë‹´ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” LangGraph ì—ì´ì „íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
    - STT transcriptë¥¼ ë°›ì•„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ëˆ„ì 
    - LLMì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ëŒ€í™” ìš”ì•½ ìƒì„±
    - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì—…ë°ì´íŠ¸ ì¦‰ì‹œ ë°˜í™˜
    - Runtime Context íŒ¨í„´ìœ¼ë¡œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ í•œ ë²ˆë§Œ ì„¤ì •

Architecture:
    StateGraph with Runtime Context:
        START â†’ summarize_node â†’ END
        - Runtime Context: ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì—ì´ì „íŠ¸ ìƒì„± ì‹œ ê³ ì •
        - MessagesState: ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ìë™ ê´€ë¦¬

State Structure:
    - room_name: ë°© ì´ë¦„
    - conversation_history: [(speaker_name, text, timestamp)]
    - current_summary: í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½
    - messages: MessagesStateê°€ ìë™ ê´€ë¦¬í•˜ëŠ” ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬

Example:
    >>> graph = create_agent_graph(llm)
    >>> async for chunk in graph.astream(
    ...     state,
    ...     stream_mode="updates",
    ...     context={"system_message": "ê³ ê° ìƒë‹´ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”."}
    ... ):
    ...     print(chunk)  # {"summarize": {"current_summary": "..."}}
"""
import logging
from typing import List, Dict, Any
from dataclasses import dataclass
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import MessagesState
from langgraph.runtime import Runtime
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import SystemMessage, HumanMessage

logger = logging.getLogger(__name__)


@dataclass
class ContextSchema:
    """ì—ì´ì „íŠ¸ ìƒì„± ì‹œ ì„¤ì •í•˜ëŠ” Runtime Context.

    Attributes:
        system_message (str | None): ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—ì´ì „íŠ¸ ìƒì„± ì‹œ ê³ ì •)
    """
    system_message: str | None = None


class ConversationState(MessagesState):
    """ëŒ€í™” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” State (MessagesState ìƒì†).

    MessagesState ê¸°ë³¸ í•„ë“œ:
        messages: ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ (ìë™ ê´€ë¦¬)

    ì¶”ê°€ ì»¤ìŠ¤í…€ í•„ë“œ:
        room_name (str): ë°© ì´ë¦„ (ì„¸ì…˜ ì‹ë³„ìš©)
        conversation_history (List[Dict]): ëŒ€í™” íˆìŠ¤í† ë¦¬
            ê° í•­ëª©: {"speaker_name": str, "text": str, "timestamp": float}
        current_summary (str): í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½
    """
    room_name: str
    conversation_history: List[Dict[str, Any]]
    current_summary: str


def create_summarize_node(llm: BaseChatModel):
    """LLMì„ ì‚¬ìš©í•˜ëŠ” summarize ë…¸ë“œ íŒ©í† ë¦¬ í•¨ìˆ˜.

    Runtime Context íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ìë™ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.

    Args:
        llm (BaseChatModel): ì´ˆê¸°í™”ëœ LLM ì¸ìŠ¤í„´ìŠ¤

    Returns:
        Callable: summarize_node í•¨ìˆ˜ (LLMì„ í´ë¡œì €ë¡œ ìº¡ì²˜)
    """
    async def summarize_node(
        state: ConversationState,
        runtime: Runtime[ContextSchema]
    ) -> Dict[str, Any]:
        """ëŒ€í™” ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ (Runtime Context íŒ¨í„´).

        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ LLMì„ í†µí•´ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
        Runtime Contextì—ì„œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ìë™ìœ¼ë¡œ ê°€ì ¸ì™€ ì ìš©í•©ë‹ˆë‹¤.

        Args:
            state (ConversationState): í˜„ì¬ ëŒ€í™” ìƒíƒœ
            runtime (Runtime[ContextSchema]): Runtime context (ì‹œìŠ¤í…œ ë©”ì‹œì§€ í¬í•¨)

        Returns:
            Dict[str, Any]: {
                "messages": [AIMessage],  # LLM ì‘ë‹µ ë©”ì‹œì§€
                "current_summary": str    # ìš”ì•½ í…ìŠ¤íŠ¸
            }

        Raises:
            Exception: LLM ìš”ì•½ ìƒì„± ì‹¤íŒ¨ ì‹œ

        Note:
            - ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ìš”ì•½ ë°˜í™˜
            - ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” runtime.context.system_messageì—ì„œ ìë™ìœ¼ë¡œ ê°€ì ¸ì˜´
        """
        logger.info("ğŸ”µ summarize_node started")
        conversation_history = state.get("conversation_history", [])
        logger.info(f"ğŸ“š Conversation history length: {len(conversation_history)}")

        if not conversation_history:
            logger.warning("âš ï¸ No conversation history, returning empty summary")
            return {
                "messages": [],
                "current_summary": ""
            }

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        formatted_conversation = []
        for entry in conversation_history:
            speaker = entry.get("speaker_name", "Unknown")
            text = entry.get("text", "")
            formatted_conversation.append(f"{speaker}: {text}")

        conversation_text = "\n".join(formatted_conversation)

        logger.info(f"ğŸ“Š Generating summary for {len(conversation_history)} messages")
        logger.info(f"ğŸ“ Conversation text: {conversation_text[:200]}...")

        # ì‚¬ìš©ì ë©”ì‹œì§€ ìƒì„±
        user_msg = HumanMessage(content=conversation_text)
        messages = [user_msg]

        # Runtime Contextì—ì„œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
        if (system_message := runtime.context.system_message):
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ messages ì•ì— ì¶”ê°€
            messages = [SystemMessage(system_message)] + messages
            logger.info("ğŸ“ System message added from runtime context")
        else:
            logger.warning("âš ï¸ No system message in runtime context")

        # LLM í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë°)
        logger.info("â³ Calling LLM for summary...")
        summary_chunks = []
        first_chunk_received = False

        async for chunk in llm.astream(messages):
            if not first_chunk_received:
                logger.info("âš¡ First token received (streaming started)!")
                first_chunk_received = True

            # ì²­í¬ ì „ì²´ êµ¬ì¡° ë””ë²„ê¹… (reasoning ëª¨ë¸ ë¶„ì„ìš©)
            logger.debug(f"ğŸ” Chunk: {chunk}")
            logger.debug(f"ğŸ” Chunk.__dict__: {chunk.__dict__ if hasattr(chunk, '__dict__') else 'N/A'}")

            # additional_kwargs í™•ì¸
            if hasattr(chunk, 'additional_kwargs'):
                logger.debug(f"ğŸ” additional_kwargs: {chunk.additional_kwargs}")

            if hasattr(chunk, 'content') and chunk.content:
                logger.debug(f"ğŸ“ Appending content: {chunk.content}")
                summary_chunks.append(chunk.content)

        summary = "".join(summary_chunks).strip()
        logger.info(f"âœ… Summary generated ({len(summary_chunks)} chunks): {summary[:100]}...")

        # MessagesStateê°€ ìë™ìœ¼ë¡œ messagesë¥¼ ê´€ë¦¬
        return {
            "messages": [HumanMessage(content=conversation_text)],  # ëŒ€í™” ë‚´ìš© ì €ì¥
            "current_summary": summary
        }

    return summarize_node


def create_agent_graph(llm: BaseChatModel) -> StateGraph:
    """ì‹¤ì‹œê°„ ìš”ì•½ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (Runtime Context íŒ¨í„´).

    Args:
        llm (BaseChatModel): ì´ˆê¸°í™”ëœ LLM ì¸ìŠ¤í„´ìŠ¤

    Returns:
        StateGraph: ì»´íŒŒì¼ëœ LangGraph ì¸ìŠ¤í„´ìŠ¤ (Runtime Context ì§€ì›)

    Graph Structure:
        START â†’ summarize_node â†’ END

    Runtime Context:
        - ContextSchemaë¥¼ í†µí•´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì—ì´ì „íŠ¸ ìƒì„± ì‹œ ê³ ì •
        - graph.astream(..., context={"system_message": "..."})ë¡œ ì „ë‹¬

    Example:
        >>> graph = create_agent_graph(llm)
        >>> async for chunk in graph.astream(
        ...     state,
        ...     stream_mode="updates",
        ...     context={"system_message": "ê³ ê° ìƒë‹´ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”."}
        ... ):
        ...     print(chunk)
    """
    # StateGraph ìƒì„± (context_schema ì§€ì •)
    graph = StateGraph(
        ConversationState,
        context_schema=ContextSchema  # Runtime Context íŒ¨í„´ ì ìš©
    )

    # LLMì„ ì‚¬ìš©í•˜ëŠ” summarize ë…¸ë“œ ìƒì„±
    summarize_node = create_summarize_node(llm)

    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("summarize", summarize_node)

    # ì—£ì§€ ì—°ê²°
    graph.add_edge(START, "summarize")
    graph.add_edge("summarize", END)

    # ì»´íŒŒì¼
    compiled_graph = graph.compile()

    logger.info("âœ… Agent graph created and compiled with Runtime Context support")

    return compiled_graph