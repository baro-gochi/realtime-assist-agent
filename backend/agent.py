"""LangGraph Agent for Real-time Conversation Summarization.

ì´ ëª¨ë“ˆì€ ì‹¤ì‹œê°„ ìƒë‹´ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” LangGraph ì—ì´ì „íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
    - STT transcriptë¥¼ ë°›ì•„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ëˆ„ì 
    - LLMì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ëŒ€í™” ìš”ì•½ ìƒì„±
    - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì—…ë°ì´íŠ¸ ì¦‰ì‹œ ë°˜í™˜
    - Runtime Context íŒ¨í„´ìœ¼ë¡œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ í•œ ë²ˆë§Œ ì„¤ì •

Architecture:
    StateGraph with Runtime Context:
        START â†’ summarize_node â†’ END
        - Runtime Context: ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì—ì´ì „íŠ¸ ìƒì„± ì‹œ ê³ ì •
        - MessagesState: ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ ìë™ ê´€ë¦¬

State Structure:
    - room_name: ë°© ì´ë¦„
    - conversation_history: [(speaker_name, text, timestamp)]
    - current_summary: í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½
    - messages: MessagesStateê°€ ìë™ ê´€ë¦¬í•˜ëŠ” ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬

Example:
    >>> graph = create_agent_graph(llm)
    >>> async for chunk in graph.astream(
    ...     state,
    ...     stream_mode="updates",
    ...     context={"system_message": "ê³ ê° ìƒë‹´ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”."}
    ... ):
    ...     print(chunk)  # {"summarize": {"current_summary": "..."}}
"""
import logging
from typing import List, Dict, Any
from dataclasses import dataclass
from langgraph.graph import StateGraph, START, END
from langgraph.graph.message import MessagesState
from langgraph.runtime import Runtime
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import SystemMessage, HumanMessage

logger = logging.getLogger(__name__)


@dataclass
class ContextSchema:
    """ì—ì´ì „íŠ¸ ìƒì„± ì‹œ ì„¤ì •í•˜ëŠ” Runtime Context.

    Attributes:
        system_message (str | None): ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì—ì´ì „íŠ¸ ìƒì„± ì‹œ ê³ ì •)
    """
    system_message: str | None = None


class ConversationState(MessagesState):
    """ëŒ€í™” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” State (MessagesState ìƒì†).

    MessagesState ê¸°ë³¸ í•„ë“œ:
        messages: ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ (ìë™ ê´€ë¦¬)

    ì¶”ê°€ ì»¤ìŠ¤í…€ í•„ë“œ:
        room_name (str): ë°© ì´ë¦„ (ì„¸ì…˜ ì‹ë³„ìš©)
        conversation_history (List[Dict]): ëŒ€í™” íˆìŠ¤í† ë¦¬
            ê° í•­ëª©: {"speaker_name": str, "text": str, "timestamp": float}
        current_summary (str): í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½ (JSON í˜•ì‹)
        last_summarized_index (int): ë§ˆì§€ë§‰ìœ¼ë¡œ ìš”ì•½ëœ transcript ì¸ë±ìŠ¤
    """
    room_name: str
    conversation_history: List[Dict[str, Any]]
    current_summary: str
    last_summarized_index: int


def create_summarize_node(llm: BaseChatModel):
    """LLMì„ ì‚¬ìš©í•˜ëŠ” summarize ë…¸ë“œ íŒ©í† ë¦¬ í•¨ìˆ˜.

    ì¦ë¶„ ìš”ì•½ íŒ¨í„´: ìƒˆë¡œìš´ transcriptë§Œ ì²˜ë¦¬í•˜ì—¬ ê¸°ì¡´ ìš”ì•½ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
    JSON í˜•ì‹ìœ¼ë¡œ ì—„ê²©í•˜ê²Œ ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        llm (BaseChatModel): ì´ˆê¸°í™”ëœ LLM ì¸ìŠ¤í„´ìŠ¤

    Returns:
        Callable: summarize_node í•¨ìˆ˜ (LLMì„ í´ë¡œì €ë¡œ ìº¡ì²˜)
    """
    async def summarize_node(
        state: ConversationState,
        runtime: Runtime[ContextSchema]
    ) -> Dict[str, Any]:
        """ëŒ€í™” ìš”ì•½ì„ ì¦ë¶„ ìƒì„±í•˜ëŠ” ë…¸ë“œ.

        ì´ì „ì— ìš”ì•½ëœ ë¶€ë¶„ì€ ê±´ë„ˆë›°ê³ , ìƒˆë¡œìš´ transcriptë§Œ ì²˜ë¦¬í•˜ì—¬
        ê¸°ì¡´ ìš”ì•½ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.

        Args:
            state (ConversationState): í˜„ì¬ ëŒ€í™” ìƒíƒœ
            runtime (Runtime[ContextSchema]): Runtime context (ì‹œìŠ¤í…œ ë©”ì‹œì§€ í¬í•¨)

        Returns:
            Dict[str, Any]: {
                "current_summary": str (JSON í˜•ì‹),
                "last_summarized_index": int
            }
        """
        logger.info("ğŸ”µ summarize_node started (incremental mode)")
        conversation_history = state.get("conversation_history", [])
        last_summarized_index = state.get("last_summarized_index", 0)
        current_summary = state.get("current_summary", "")

        total_count = len(conversation_history)
        logger.info(f"ğŸ“š Total history: {total_count}, Last summarized: {last_summarized_index}")

        # ìƒˆë¡œìš´ transcriptê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ìš”ì•½ ë°˜í™˜
        if last_summarized_index >= total_count:
            logger.info("â­ï¸ No new transcripts, returning existing summary")
            return {
                "current_summary": current_summary,
                "last_summarized_index": last_summarized_index
            }

        # ìƒˆë¡œìš´ transcriptë§Œ ì¶”ì¶œ
        new_transcripts = conversation_history[last_summarized_index:]
        logger.info(f"ğŸ“ Processing {len(new_transcripts)} new transcripts")

        # ìƒˆë¡œìš´ ëŒ€í™”ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        formatted_new = []
        for entry in new_transcripts:
            speaker = entry.get("speaker_name", "Unknown")
            text = entry.get("text", "")
            formatted_new.append(f"{speaker}: {text}")
        new_conversation_text = "\n".join(formatted_new)

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±: ê¸°ì¡´ ìš”ì•½ + ìƒˆ ëŒ€í™”
        if current_summary:
            user_content = f"""ê¸°ì¡´ ìš”ì•½:
{current_summary}

ìƒˆë¡œìš´ ëŒ€í™”:
{new_conversation_text}

ìœ„ ê¸°ì¡´ ìš”ì•½ì— ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬ ì—…ë°ì´íŠ¸ëœ ìš”ì•½ì„ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”."""
        else:
            user_content = f"""ëŒ€í™”:
{new_conversation_text}

ìœ„ ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ì—¬ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”."""

        user_msg = HumanMessage(content=user_content)
        messages = [user_msg]

        # Runtime Contextì—ì„œ ì‹œìŠ¤í…œ ë©”ì‹œì§€ ê°€ì ¸ì˜¤ê¸°
        if (system_message := runtime.context.system_message):
            messages = [SystemMessage(system_message)] + messages
            logger.info("ğŸ“ System message added from runtime context")

        # LLM í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ì—†ì´ í•œ ë²ˆì—)
        logger.info("â³ Calling LLM for incremental summary...")
        try:
            response = await llm.ainvoke(messages)
            summary = response.content.strip()
            logger.info(f"âœ… Summary generated: {summary[:100]}...")
        except Exception as e:
            logger.error(f"âŒ LLM call failed: {e}")
            # ì—ëŸ¬ ì‹œ ê¸°ì¡´ ìš”ì•½ ìœ ì§€
            return {
                "current_summary": current_summary,
                "last_summarized_index": last_summarized_index
            }

        # ìƒˆë¡œìš´ ì¸ë±ìŠ¤ë¡œ ì—…ë°ì´íŠ¸
        new_last_index = total_count

        return {
            "current_summary": summary,
            "last_summarized_index": new_last_index
        }

    return summarize_node


def create_agent_graph(llm: BaseChatModel) -> StateGraph:
    """ì‹¤ì‹œê°„ ìš”ì•½ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (Runtime Context íŒ¨í„´).

    Args:
        llm (BaseChatModel): ì´ˆê¸°í™”ëœ LLM ì¸ìŠ¤í„´ìŠ¤

    Returns:
        StateGraph: ì»´íŒŒì¼ëœ LangGraph ì¸ìŠ¤í„´ìŠ¤ (Runtime Context ì§€ì›)

    Graph Structure:
        START â†’ summarize_node â†’ END

    Runtime Context:
        - ContextSchemaë¥¼ í†µí•´ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¥¼ ì—ì´ì „íŠ¸ ìƒì„± ì‹œ ê³ ì •
        - graph.astream(..., context={"system_message": "..."})ë¡œ ì „ë‹¬

    Example:
        >>> graph = create_agent_graph(llm)
        >>> async for chunk in graph.astream(
        ...     state,
        ...     stream_mode="updates",
        ...     context={"system_message": "ê³ ê° ìƒë‹´ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ì„¸ìš”."}
        ... ):
        ...     print(chunk)
    """
    # StateGraph ìƒì„± (context_schema ì§€ì •)
    graph = StateGraph(
        ConversationState,
        context_schema=ContextSchema  # Runtime Context íŒ¨í„´ ì ìš©
    )

    # LLMì„ ì‚¬ìš©í•˜ëŠ” summarize ë…¸ë“œ ìƒì„±
    summarize_node = create_summarize_node(llm)

    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("summarize", summarize_node)

    # ì—£ì§€ ì—°ê²°
    graph.add_edge(START, "summarize")
    graph.add_edge("summarize", END)

    # ì»´íŒŒì¼
    compiled_graph = graph.compile()

    logger.info("âœ… Agent graph created and compiled with Runtime Context support")

    return compiled_graph