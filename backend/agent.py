"""LangGraph Agent for Real-time Conversation Summarization.

ì´ ëª¨ë“ˆì€ ì‹¤ì‹œê°„ ìƒë‹´ ëŒ€í™”ë¥¼ ìš”ì•½í•˜ëŠ” LangGraph ì—ì´ì „íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
    - STT transcriptë¥¼ ë°›ì•„ ëŒ€í™” íˆìŠ¤í† ë¦¬ ëˆ„ì 
    - LLMì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì‹œê°„ ëŒ€í™” ìš”ì•½ ìƒì„±
    - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œë¡œ ì—…ë°ì´íŠ¸ ì¦‰ì‹œ ë°˜í™˜

Architecture:
    StateGraph:
        START â†’ summarize_node â†’ END

State Structure:
    - room_name: ë°© ì´ë¦„
    - conversation_history: [(speaker_name, text, timestamp)]
    - current_summary: í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½

Example:
    >>> state = {
    ...     "room_name": "ìƒë‹´ì‹¤1",
    ...     "conversation_history": [
    ...         {"speaker_name": "ê³ ê°", "text": "í™˜ë¶ˆí•˜ê³  ì‹¶ì–´ìš”", "timestamp": 1234567890.0}
    ...     ],
    ...     "current_summary": ""
    ... }
    >>> async for chunk in graph.astream(state, stream_mode="updates"):
    ...     print(chunk)  # {"summarize": {"current_summary": "..."}}
"""
import logging
from typing import List, Dict, Any, Callable
from langgraph.graph import StateGraph, START, END
from langchain_core.language_models.chat_models import BaseChatModel
from langgraph.graph.message import MessagesState

logger = logging.getLogger(__name__)

class ConversationState(MessagesState):
    """ëŒ€í™” ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ëŠ” State.

    Attributes:
        room_name (str): ë°© ì´ë¦„ (ì„¸ì…˜ ì‹ë³„ìš©)
        conversation_history (List[Dict]): ëŒ€í™” íˆìŠ¤í† ë¦¬
            ê° í•­ëª©: {"speaker_name": str, "text": str, "timestamp": float}
        current_summary (str): í˜„ì¬ê¹Œì§€ì˜ ëŒ€í™” ìš”ì•½
    """
    room_name: str
    conversation_history: List[Dict[str, Any]]
    current_summary: str


def create_summarize_node(llm: BaseChatModel) -> Callable:
    """LLMì„ ì‚¬ìš©í•˜ëŠ” summarize ë…¸ë“œ íŒ©í† ë¦¬ í•¨ìˆ˜.

    Args:
        llm (BaseChatModel): ì´ˆê¸°í™”ëœ LLM ì¸ìŠ¤í„´ìŠ¤

    Returns:
        Callable: summarize_node í•¨ìˆ˜ (LLMì„ í´ë¡œì €ë¡œ ìº¡ì²˜)
    """
    async def summarize_node(state: ConversationState) -> Dict[str, str]:
        """ëŒ€í™” ìš”ì•½ì„ ìƒì„±í•˜ëŠ” ë…¸ë“œ.

        ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ë¶„ì„í•˜ì—¬ LLMì„ í†µí•´ ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
        ì‹¤ì‹œê°„ ìƒë‹´ ìƒí™©ì— ë§ì¶° ê°„ê²°í•˜ê³  í•µì‹¬ì ì¸ ìš”ì•½ì„ ì œê³µí•©ë‹ˆë‹¤.

        Args:
            state (ConversationState): í˜„ì¬ ëŒ€í™” ìƒíƒœ

        Returns:
            Dict[str, str]: {"current_summary": "ëŒ€í™” ìš”ì•½ í…ìŠ¤íŠ¸"}

        Raises:
            Exception: LLM ìš”ì•½ ìƒì„± ì‹¤íŒ¨ ì‹œ

        Note:
            - ëŒ€í™” íˆìŠ¤í† ë¦¬ê°€ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ ìš”ì•½ ë°˜í™˜
        """
        logger.info("ğŸ”µ summarize_node started")
        conversation_history = state.get("conversation_history", [])
        logger.info(f"ğŸ“š Conversation history length: {len(conversation_history)}")

        if not conversation_history:
            logger.warning("âš ï¸ No conversation history, returning empty summary")
            return {"current_summary": ""}

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        formatted_conversation = []
        for entry in conversation_history:
            speaker = entry.get("speaker_name", "Unknown")
            text = entry.get("text", "")
            formatted_conversation.append(f"{speaker}: {text}")

        full_text = "\n".join(formatted_conversation)

        logger.info(f"ğŸ“Š Generating summary for {len(conversation_history)} messages")
        logger.info(f"ğŸ“ Full text to summarize: {full_text[:200]}...")

        # LLM ìš”ì•½ ìƒì„± (ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ)
        summary = await _generate_llm_summary(llm, full_text)
        logger.info(f"âœ… Summary generated: {summary[:100]}...")

        return {"current_summary": summary}

    return summarize_node


async def _generate_llm_summary(llm: BaseChatModel, text: str) -> str:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ëŒ€í™” ìš”ì•½ ìƒì„±.

    Args:
        llm (BaseChatModel): LLM ì¸ìŠ¤í„´ìŠ¤
        text (str): ìš”ì•½í•  ëŒ€í™” í…ìŠ¤íŠ¸

    Returns:
        str: ìƒì„±ëœ ìš”ì•½ í…ìŠ¤íŠ¸

    Raises:
        Exception: LLM í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ
    """
    logger.info("ğŸ”µ _generate_llm_summary started")
    logger.info(f"ğŸ“ Text length: {len(text)} characters")

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ëŠ” LLM ì´ˆê¸°í™” ì‹œ bindë˜ì—ˆìœ¼ë¯€ë¡œ, ëŒ€í™” ë‚´ìš©ë§Œ ì „ë‹¬
    # â†’ í† í° ìˆ˜ ê°ì†Œ + ì‘ë‹µ ì†ë„ í–¥ìƒ
    messages = [
        {
            "role": "user",
            "content": f"{text}"  # ë¶ˆí•„ìš”í•œ "ë‹¤ìŒ ëŒ€í™”ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”" ì œê±°
        }
    ]

    logger.info(f"ğŸ“¤ Sending streaming request to LLM (system prompt already bound)")

    try:
        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìœ¼ë¡œ ì²« í† í°ì„ ë¹ ë¥´ê²Œ ë°›ìŒ
        logger.info("â³ Calling llm.astream() for faster response...")

        summary_chunks = []
        first_chunk_received = False

        async for chunk in llm.astream(messages):
            if not first_chunk_received:
                logger.info("âš¡ First token received (streaming started)!")
                first_chunk_received = True

            if hasattr(chunk, 'content') and chunk.content:
                summary_chunks.append(chunk.content)

        summary = "".join(summary_chunks).strip()
        logger.info(f"âœ… LLM summary generated: {summary[:100]}...")
        logger.info(f"ğŸ“Š Summary length: {len(summary)} characters")
        return summary

    except Exception as e:
        logger.error(f"âŒ LLM API call failed: {type(e).__name__}: {str(e)}", exc_info=True)
        raise


def create_agent_graph(llm: BaseChatModel) -> StateGraph:
    """ì‹¤ì‹œê°„ ìš”ì•½ ì—ì´ì „íŠ¸ ê·¸ë˜í”„ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        llm (BaseChatModel): ì´ˆê¸°í™”ëœ LLM ì¸ìŠ¤í„´ìŠ¤

    Returns:
        StateGraph: ì»´íŒŒì¼ëœ LangGraph ì¸ìŠ¤í„´ìŠ¤

    Graph Structure:
        START â†’ summarize_node â†’ END

    Example:
        >>> graph = create_agent_graph(llm)
        >>> async for chunk in graph.astream(state, stream_mode="updates"):
        ...     print(chunk)
    """
    # StateGraph ìƒì„±
    graph = StateGraph(ConversationState)

    # LLMì„ ì‚¬ìš©í•˜ëŠ” summarize ë…¸ë“œ ìƒì„±
    summarize_node = create_summarize_node(llm)

    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("summarize", summarize_node)

    # ì—£ì§€ ì—°ê²°
    graph.add_edge(START, "summarize")
    graph.add_edge("summarize", END)

    # ì»´íŒŒì¼
    compiled_graph = graph.compile()

    logger.info("âœ… Agent graph created and compiled")

    return compiled_graph